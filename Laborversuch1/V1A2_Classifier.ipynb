{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning \n",
    "predict() erklärt: https://heartbeat.fritz.ai/guide-to-implementing-k-nearest-neighbors-in-your-machine-learning-model-3b8420f16d93 <br>\n",
    "Datenmatrix X = ([1,2,3][2,3,4][3,4,5][4,5,6])<br>\n",
    "Klassenlabels T = ([0,1,0,1])<br>\n",
    "k = anzahl der nächsten Nachbarn die gesucht werden sollen<br>\n",
    "Testvektor x = der zu klassifizierende Vektor<br>\n",
    "\n",
    "die Mehrheit der Nachbarn mit einem Label die Testvektor x am nächsten sind  bestimmen das Label (predict) von Testvektor x.<br>\n",
    "sprich: X[0] = T[0]; X[1] = T[1]; etc...\n",
    "unsere Datenmatrix X ist schon zugeordnet zu den Klassenlabels T."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)  \n",
    "Welche Klassen gehören zu dem Modul und welchen Zweck haben sie jeweils?\n",
    "   - Classifier: \n",
    "       - Abstrakte Basisklasse für einen Klassifikator.\n",
    "        \n",
    "   - KNNClassifier:\n",
    "       - berechnet KNN und liefert wahrscheinliche Klasse, a-posteriori-Verteilung und Indizes der                             nächstgelegenen Nachbarn. Erbt von 'Classifier'\n",
    "        \n",
    "   - FastKNNClassifier:\n",
    "       - berechnet KNN basierend auf kd-trees und liefert Indizes von k nächstgelegenen Nachbarn. Erbt von                     KNNClassifier.\n",
    "       \n",
    "   - Wozu dienen jeweils die Methoden \\__init__(self,C), fit(self,X,T), predict(self,x) und crossvalidate(self,S,X,T)      der Basisklasse 'Classifier'?\n",
    "       -  \\__init__(self,C) ist der Konstruktor der Klasse und setzt für jede Instanz C (:= Anzahl der Klassen)                  standardmäßig auf 2.\n",
    "       - fit(self,X,T) trainiert den Klassifikator wobei es T (:=Label), X(:=DatenVektor).\n",
    "       - predict(self,x) liefert Label der wahrscheinlichsten Klasse für Testvektor x.\n",
    "       - crossvalidate(self,S,X,T) macht eine Kreuzvalidierung und liefert Fehlerwahrscheinlichkeiten.\n",
    "      \n",
    "## b) \n",
    "Wie \"lernt\" ein k-NN-Klassifikator? Beschreiben Sie kurz was die Methode fit(self,X,T) macht. <br>\n",
    "    Legt die Matrix an welche die Vektoren beihnahltet und die zugehörigen Labels.\n",
    "\n",
    "## c) \n",
    "Ergebnisse mit Testdaten:<br>\n",
    "   \n",
    "k=3\n",
    "   - nearest neighbors: idx_knn= [2 1 3]\n",
    "   - Test vector is most likely from class  1\n",
    "   - A-Posteriori Class Distribution: prob(x is from class i)= [0.3333333333333333, 0.6666666666666666]\n",
    "   \n",
    "k=2\n",
    "   <br>\n",
    "   - nearest neighbors: idx_knn= [2 1]\n",
    "   - Test vector is most likely from class  0\n",
    "   - A-Posteriori Class Distribution: prob(x is from class i)= [0.5, 0.5]\n",
    "   \n",
    "k=1\n",
    "   <br>\n",
    "   - nearest neighbors: idx_knn= [2]\n",
    "   - Test vector is most likely from class  0\n",
    "   - A-Posteriori Class Distribution: prob(x is from class i)= [1.0, 0.0]\n",
    "      <br>   <br>\n",
    " Es sollte für C=2 immer ein ungerades 'k' gewählt werden, da sonst die Wahrscheinlichkeit bei 50:50 liegen kann bzw. keine Wahrscheinlichkeit für eine Bestimmung einer Klasse überwiegt.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA                # neu\n",
    "import scipy.spatial\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------- \n",
    "# Base class for classifiers\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "class Classifier:\n",
    "    \"\"\"\n",
    "    Abstract base class for a classifier.\n",
    "    Inherit from this class to implement a concrete classification algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,C=2): \n",
    "        \"\"\"\n",
    "        Constructor of class Classifier\n",
    "        Should be called by the constructors of derived classes\n",
    "        :param C: Number of different classes\n",
    "        \"\"\"\n",
    "        self.C = C            # set C=number of different classes \n",
    "\n",
    "    def fit(self,X,T):    \n",
    "        \"\"\" \n",
    "        Train classier by training data X, T, should be overwritten by any derived class\n",
    "        :param X: Data matrix, contains in each row a data vector\n",
    "        :param T: Vector of class labels, must have same length as X, each label should be integer in 0,1,...,C-1\n",
    "        :returns: - \n",
    "        \"\"\"\n",
    "        shapeX,shapeT=X.shape,T.shape    # X must be a N x D matrix; T must be a N x 1 matrix; N is number of data vectors; D is dimensionality\n",
    "        assert len(shapeX)==2, \"Classifier.fit(self,X,T): X must be two-dimensional array!\"\n",
    "        assert len(shapeT)==1, \"Classifier.fit(self,X,T): T must be one-dimensional array!\"\n",
    "        assert shapeX[0]==shapeT[0], \"Classifier.fit(self,X,T): Data matrix X and class labels T must have same length!\"\n",
    "        self.C=max(T)+1;      # number of different integer-type class labels (assuming that T(i) is in {0,1,...,C-1})\n",
    "\n",
    "    def predict(self,x):\n",
    "        \"\"\" \n",
    "        Implementation of classification algorithm, should be overwritten in any derived class\n",
    "        :param x: test data vector\n",
    "        :returns: label of most likely class that test vector x belongs to (and possibly additional information)\n",
    "        \"\"\"\n",
    "        return -1,None,None\n",
    "\n",
    "    def crossvalidate(self,S,X,T):    # do a S-fold cross validation \n",
    "        \"\"\"\n",
    "        Do a S-fold cross validation\n",
    "        :param S: Number of parts the data set is divided into\n",
    "        :param X: Data matrix (one data vector per row)\n",
    "        :param T: Vector of class labels; T[n] is label of X[i]\n",
    "        :returns pClassError: probability of a classification error (=1-Accuracy)\n",
    "        :returns pConfErrors: confusion matrix, pConfErrors[i,j] is the probability that a vector from true class j will be mis-classified as class i\n",
    "        \"\"\"\n",
    "        N=len(X)                                            # N=number of data vectors\n",
    "        perm = np.random.permutation(N)                     # do a random permutation of X and T...\n",
    "        Xp,Tp=[X[i] for i in perm], [T[i] for i in perm]    # ... to get a random partition of the data set\n",
    "        idxS = [range(i*N//S,(i+1)*N//S) for i in range(S)] # divide data set into S parts:\n",
    "        C=max(T)+1;                                         # number of different class labels (assuming that t is in {0,1,...,C-1})\n",
    "        nC          = np.zeros(C)                           # initialize class probabilities: nC[i]:=N*pr[xn is of class i]\n",
    "        pConfErrors = np.zeros((C,C))                       # initialize confusion error probabilities pr[class i|class j]\n",
    "        pClassError = 0                                     # initialize probability of a classification error\n",
    "        for idxTest in idxS:                                # loop over all possible test data sets\n",
    "            # (i) generate training and testing data sets and train classifier        \n",
    "            idxLearn = [i for i in range(N) if i not in idxTest]                      # remaining indices (not in idxTest) are learning data\n",
    "            if(S<=1): idxLearn=idxTest                                                # if S==1 use entire data set for learning and testing\n",
    "            X_learn, T_learn = [Xp[i] for i in idxLearn], [Tp[i] for i in idxLearn]   # learning data for training the classifier\n",
    "            X_test , T_test  = [Xp[i] for i in idxTest] , [Tp[i] for i in idxTest]    # test data \n",
    "            self.fit(np.array(X_learn),np.array(T_learn))                             # train classifier\n",
    "            # (ii) test classifier\n",
    "            for i in range(len(X_test)):  # loop over all data vectors to be tested\n",
    "                # (ii.a) classify i-th test vector\n",
    "                t_test = self.predict(X_test[i])[0]             # classify test vector\n",
    "                # (ii.b) check for classification errors\n",
    "                t_true = T_test[i]                              # true class label\n",
    "                nC[t_true]=nC[t_true]+1                         # count occurrences of individual classes\n",
    "                pConfErrors[t_test][t_true]=pConfErrors[t_test][t_true]+1  # count conditional class errors\n",
    "                if(t_test!=t_true): pClassError=pClassError+1              # count total number of errors\n",
    "        pClassError=float(pClassError)/float(N)         # probability of a classification error\n",
    "        for i in range(C): \n",
    "            for j in range(C): \n",
    "                pConfErrors[i,j]=float(pConfErrors[i,j])/float(nC[j])   # finally compute confusion error probabilities\n",
    "        self.pClassError,self.pConfErrors=pClassError,pConfErrors       # store error probabilities as object fields\n",
    "        return pClassError, pConfErrors                 # return error probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------- \n",
    "# (Naive) k-nearest-neighbor classifier based on simple look-up-table and exhaustive search\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "class KNNClassifier(Classifier):\n",
    "    \"\"\"\n",
    "    (Naive) k-nearest-neighbor classifier based on simple look-up-table and exhaustive search\n",
    "    Derived from base class Classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,C=1,k=1):\n",
    "        \"\"\"\n",
    "        Constructor of the KNN-Classifier\n",
    "        :param C: Number of different classes\n",
    "        :param k: Number of nearest neighbors that classification is based on\n",
    "        \"\"\"\n",
    "        Classifier.__init__(self,C) # call constructor of base class  \n",
    "        self.k = k                  # k is number of nearest-neighbors used for majority decision\n",
    "        self.X, self.T = [],[]      # initially no data is stored\n",
    "\n",
    "    def fit(self,X,T):\n",
    "        \"\"\"\n",
    "        Train classifier; for naive KNN Classifier this just means to store data matrix X and label vector T\n",
    "        :param X: Data matrix, contains in each row a data vector\n",
    "        :param T: Vector of class labels, must have same length as X, each label should be integer in 0,1,...,C-1\n",
    "        :returns: - \n",
    "        \"\"\"\n",
    "        Classifier.fit(self,X,T);   # call to base class to check for matrix dimensions etc.\n",
    "        self.X, self.T = X,T        # just store the N x D data matrix and the N x 1 label matrix (N is number and D dimensionality of data vectors) \n",
    "        \n",
    "    def getKNearestNeighbors(self, x, k=None, X=None):\n",
    "        \"\"\"\n",
    "        compute the k nearest neighbors for a query vector x given a data matrix X\n",
    "        :param x: the query vector x\n",
    "        :param X: the N x D data matrix (in each row there is data vector) as a numpy array\n",
    "        :param k: number of nearest-neighbors to be returned\n",
    "        :return: list of k line indexes referring to the k nearest neighbors of x in X\n",
    "        \"\"\"\n",
    "        if(k==None): k=self.k                      # per default use stored k \n",
    "        if(X==None): X=self.X                      # per default use stored X\n",
    "\n",
    "        d = [(LA.norm(X[i]-x)**2) for i in range(len(X))]\n",
    "        idxK = np.argsort(d)\n",
    "        return idxK[:k]                            # OK REPLACE: Insert/adapt your code from V1A1_KNearestNeighborSearch.py\n",
    "\n",
    "    def predict(self,x,k=None):\n",
    "        \"\"\" \n",
    "        Implementation of classification algorithm, should be overwritten in any derived classes\n",
    "        :param x: test data vector\n",
    "        :param k: search k nearest neighbors (default self.k)\n",
    "        :returns prediction: label of most likely class that test vector x belongs to\n",
    "                             if there are two or more classes with maximum probability then one class is chosen randomly\n",
    "        :returns pClassPosteriori: A-Posteriori probabilities, pClassPosteriori[i] is probability that x belongs to class i\n",
    "        :returns idxKNN: indexes of the k nearest neighbors (ordered w.r.t. descending distance) \n",
    "        \"\"\"\n",
    "        idxKNN = self.getKNearestNeighbors(x,k)    # get indexes of K nearest neighbors of x\n",
    "        #prediction=0                               # OK REPLACE DUMMY CODE BY YOUR OWN CODE!\n",
    "        #prediction=self.T[idxKNN[0]]\n",
    "\n",
    "        checkSet = set(self.T) # eine set mit jedem label (jedes label steht (nur) einmal in der set)\n",
    "        countedLabels = [] # jedes vorkommen eines labels wird in 'countedLabels' geschrieben\n",
    "        for i in idxKNN:\n",
    "            countedLabels.append(self.T[i]) \n",
    "    \n",
    "        selectLabel = [] # jedes labelvorkommen wird gezählt und in 'selectLabel' geschrieben \n",
    "        for a in checkSet:\n",
    "            selectLabel.append(countedLabels.count(a))\n",
    "        \n",
    "        prediction = self.T[selectLabel.index(max(selectLabel))] # predict ist T am Index vom Maximum der gezählten Labels\n",
    "     \n",
    "        if selectLabel.count(max(selectLabel)) > 1: # falls mehrere Klassen maximale Wahrscheinlichkeit haben \n",
    "            item_index=[]\n",
    "            max_item = max(selectLabel)\n",
    "            for i in range(len(selectLabel)):\n",
    "                if selectLabel[i] == max_item:\n",
    "                    item_index.append(i) \n",
    "            prediction = self.T[random.choice(item_index)]\n",
    "        \n",
    "        \n",
    "        #pClassPosteriori=self.C*[1.0/self.C]       # OK REPLACE DUMMY CODE BY YOUR OWN CODE!\n",
    "        pClassPosteriori = [i/self.k for i in selectLabel] \n",
    "        \n",
    "        \n",
    "        return prediction, pClassPosteriori, idxKNN  # return predicted class, a-posteriori-distribution, and indexes of nearest neighbors\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------- \n",
    "# Fast k-nearest-neighbor classifier based on scipy KD trees\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "class FastKNNClassifier(KNNClassifier):\n",
    "    \"\"\"\n",
    "    Fast k-nearest-neighbor classifier based on kd-trees \n",
    "    Inherits from class KNNClassifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,C=1,k=1):\n",
    "        \"\"\"\n",
    "        Constructor of the KNN-Classifier\n",
    "        :param C: Number of different classes\n",
    "        :param k: Number of nearest neighbors that classification is based on\n",
    "        \"\"\"\n",
    "        KNNClassifier.__init__(self,C,k)     # call to parent class constructor  \n",
    "        self.k = k                  # k is number of nearest-neighbors used for majority decision\n",
    "        self.X, self.T = [],[]      # initially no data is stored\n",
    "\n",
    "    def fit(self,X,T):\n",
    "        \"\"\"\n",
    "        Train classifier by creating a kd-tree \n",
    "        :param X: Data matrix, contains in each row a data vector\n",
    "        :param T: Vector of class labels, must have same length as X, each label should be integer in 0,1,...,C-1\n",
    "        :returns: - \n",
    "        \"\"\"\n",
    "        KNNClassifier.fit(self,X,T)                # call to parent class method (just store X and T)\n",
    "        #self.kdtree = None                         # REPLACE DUMMY CODE BY YOUR OWN CODE! Do an indexing of the feature vectors by constructing a kd-tree\n",
    "        self.kdtree = scipy.spatial.KDTree(X)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def getKNearestNeighbors(self, x, k=None):  # realizes fast K-nearest-neighbor-search of x in data set X\n",
    "        \"\"\"\n",
    "        fast computation of the k nearest neighbors for a query vector x given a data matrix X by using the KD-tree\n",
    "        :param x: the query vector x\n",
    "        :param k: number of nearest-neighbors to be returned\n",
    "        :return idxNN: return list of k line indexes referring to the k nearest neighbors of x in X\n",
    "        \"\"\"\n",
    "        if(k==None): k=self.k                      # do a K-NN search...\n",
    "        #idxNN = k*[0]                              # REPLACE DUMMY CODE BY YOUR OWN CODE! Compute nearest neighbors using the KD-Tree\n",
    "        idxNN = self.kdtree.query(x,k)\n",
    "        \n",
    "        return idxNN[1]                               # return indexes of k nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************************************************\n",
    "# __main___\n",
    "# Module test\n",
    "# *******************************************************\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # (i) Generate dummy data \n",
    "    X = np.array([[1,2,3],[2,3,4],[3,4,5],[4,5,6]]);      # data matrix X: list of data vectors (=database) of dimension D=3\n",
    "    T = np.array([0,1,0,1]);                              # target values aka class labels\n",
    "    x = np.array([1.5,3.6,5.7]);                          # a test data vector\n",
    "    print(\"Data matrix X=\\n\",X)\n",
    "    print(\"Class labels T=\\n\",T)\n",
    "    print(\"Test vector x=\",x)\n",
    "    print(\"Euklidean distances d=\\n\",[LA.norm(X[i]-x) for i in range(len(X))]) # neu\n",
    "   # print(\"Euklidean distances d=\",[]) # <- original \n",
    "\n",
    "    # (ii) Train simple KNN-Classifier\n",
    "    knnc = KNNClassifier()         # construct kNN Classifier\n",
    "    knnc.fit(X,T)                  # train with given data\n",
    "\n",
    "    # (iii) Classify test vector x\n",
    "    k=3\n",
    "    c,pc,idx_knn=knnc.predict(x,k)\n",
    "    print(\"\\nClassification with the naive KNN-classifier:\")\n",
    "    print(\"Test vector is most likely from class \",c)\n",
    "    print(\"A-Posteriori Class Distribution: prob(x is from class i)=\",pc)\n",
    "    print(\"Indexes of the k=\",k,\" nearest neighbors: idx_knn=\",idx_knn)\n",
    "\n",
    "    # (iv) Repeat steps (ii) and (iii) for the FastKNNClassifier (based on KD-Trees)\n",
    "    # INSERT YOUR CODE\n",
    "    fknnc = FastKNNClassifier()\n",
    "    fknnc.fit(X,T)\n",
    "\n",
    "    fidx_knn=fknnc.getKNearestNeighbors(x,k)\n",
    "    print(\"\\nClassification with the FastKNN-classifier:\")\n",
    "    print(\"Indexes of the k=\",k,\" nearest neighbors: idx_knn=\",fidx_knn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
