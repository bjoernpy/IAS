{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# increase display width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 2.2\n",
    "**Thema: Python-Modul für Lineare und k-Nearest-Neighbor (KNN) Regression** <br>\n",
    "Da wir im folgenden verschiedene Regressions-Verfahren implementieren wollen lohnt es sich\n",
    "ein eigenes Python-Modul dafür zu erstellen (siehe Programmgerüst V2A2_Regression.py).\n",
    "## a)\n",
    "Versuchen Sie zunächst den Aufbau des Moduls V2A2_Regression.py zu verstehen:\n",
    "   - Betrachten Sie den Aufbau des Moduls durch Eingabe von pydoc V2A2_Regressifier. Welche Klassen gehören zu dem Modul und welchen Zweck haben sie jeweils?\n",
    "      - Regressifier: \n",
    "      - DataScaler:\n",
    "      - LSRRegressifier:\n",
    "      - KNNRegressifier:\n",
    "   - Betrachten Sie nun die Basis-Klasse Regressifier im Quelltext: Wozu dienen jeweils die Methoden fit(self,X,T), predict(self,x) und crossvalidate(self,S,X,T) ?\n",
    "   \n",
    "   - Worin unterscheidet sich crossvalidate(.) von der entsprechenden Methode für Klassifikation (siehe vorigen Versuch)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)\n",
    "Betrachten Sie nun die Funktion phi_polynomial(x,deg):\n",
    "   - Was berechnet die Funktion? Welches Ergebnis liefert phi_polynomial([3],5)? Welches Ergebnis liefert phi_polynomial([3,5],2)?\n",
    "   \n",
    "   - Geben Sie eine allgemeine Formel an für das Ergebnis von phi_polynomial([x1,x2],2)?\n",
    "   \n",
    "   - Wozu braucht man diese Funktion im Zusammenhang mit Regression?\n",
    "   \n",
    "   - Bis zu welchem Polynomgrad kann die Funktion Basisfunktionen berechnen? Erweitern Sie die Funktion mindestens bis Grad 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c)\n",
    "Betrachten Sie die Klasse LSRRegressifier:\n",
    "   - Welche Art von Regressions-Modell berechnet diese Klasse?\n",
    "   \n",
    "   - Wozu dienen jeweils die Parameter lmbda, phi, flagSTD und eps ?\n",
    "   \n",
    "   - Welche Rolle spielt hier die Klasse DataScaler?<br> In welchen Methoden und zu welchem Zweck werden die Daten ggf. umskaliert?<br> Welches Problem kann auftreten wenn man dies nicht tut? <br>Wozu braucht man die Variablen Z und maxZ in der Methode fit(.)?\n",
    "\n",
    "   - Vervollständigen Sie die Methoden fit(self,X,T,...) und predict(self,x,...) (vgl. vorige Aufgabe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d)\n",
    "Betrachten Sie die Klasse KNNRegressifier:\n",
    "   - Welche Art von Regressions-Modell berechnet diese Klasse?\n",
    "   \n",
    "   - Wozu dienen jeweils die Parameter K und flagKLinReg?\n",
    "   \n",
    "   - Beschreiben Sie kurz in eigenen Worten (2-3 Sätze) auf welche Weise die Prädiktion y(x) berechnet wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e)\n",
    "Betrachten Sie abschließend den Modultest:\n",
    "   - Beschreiben Sie kurz was im Modultest passiert.\n",
    "   \n",
    "   - Welche Gewichte W werden gelernt? Wie lautet also die gelernte Prädiktionsfunktion? Welche Funktion sollte sich idealerweise (für N ! 1) ergeben?\n",
    "   \n",
    "   - Welche Ergebnisse liefert die Kreuzvalidierung? Was bedeuten die Werte?\n",
    "   \n",
    "   - Vergleichen und Bewerten Sie die Ergebnisse von Least Squares Regression gegenüber der KNN-Regression (nach Optimierung der Hyper-Parameter $\\lambda$, K, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------- \n",
    "# base class for regressifiers\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "class Regressifier:\n",
    "    \"\"\"\n",
    "    Abstract base class for regressifiers\n",
    "    Inherit from this class to implement a concrete regression algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self,X,T):        # train/compute regression with lists of feature vectors X and class labels T\n",
    "        \"\"\"\n",
    "        Train regressifier by training data X, T, should be overwritten by any derived class\n",
    "        :param X: Data matrix of size NxD, contains in each row a data vector of size D\n",
    "        :param T: Target vector matrix of size NxK, contains in each row a target vector of size K\n",
    "        :returns: -\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self,x):      # predict a target vector given the data vector x \n",
    "        \"\"\"\n",
    "        Implementation of the regression algorithm; should be overwritten by any derived class \n",
    "        :param x: test data vector of size D\n",
    "        :returns: predicted target vector\n",
    "        \"\"\"\n",
    "        return None           \n",
    "\n",
    "    def crossvalidate(self,S,X,T,dist=lambda t: np.linalg.norm(t)):  # do a S-fold cross validation \n",
    "        \"\"\"\n",
    "        Do a S-fold cross validation\n",
    "        :param S: Number of parts the data set is divided into\n",
    "        :param X: Data matrix (one data vector per row)\n",
    "        :param T: Matrix of target vectors; T[n] is target vector of X[n]\n",
    "        :param dist: a fuction dist(t) returning the length of vector t (default=Euklidean)\n",
    "        :returns (E_dist,sd_dist,E_min,E_max) : mean, standard deviation, minimum, and maximum of absolute error \n",
    "        :returns (Erel_dist,sdrel_dist,Erel_min,Erel_max) : mean, standard deviation, minimum, and maximum of relative error \n",
    "        \"\"\"\n",
    "        X,T=np.array(X),np.array(T)                         # ensure array type\n",
    "        N=len(X)                                            # N=number of data vectors\n",
    "        perm = np.random.permutation(N)                     # do a random permutation of X and T...\n",
    "        X1,T1=[X[i] for i in perm], [T[i] for i in perm]    # ... to get random partitions of the data set\n",
    "        idxS = [range(i*N//S,(i+1)*N//S) for i in range(S)] # divide data set into S parts:\n",
    "        E_dist,E_dist2,E_max,E_min=0,0,-1,-1                # initialize first two moments of (absolute) regression error as well as max/min error \n",
    "        Erel_dist,Erel_dist2,Erel_max,Erel_min=0,0,-1,-1    # initialize first two moments of relative regression error as well as max/min error \n",
    "        for idxTest in idxS:                                # loop over all possible test data sets\n",
    "            # (i) generate training and testing data sets and train classifier        \n",
    "            idxLearn = [i for i in range(N) if i not in idxTest]                      # remaining indices (not in idxTest) are learning data\n",
    "            if(S<=1): idxLearn=idxTest                                                # if S==1 use entire data set for learning and testing\n",
    "            X_learn, T_learn = np.array([X1[i] for i in idxLearn]), np.array([T1[i] for i in idxLearn]) # learn data \n",
    "            X_test , T_test  = np.array([X1[i] for i in idxTest ]), np.array([T1[i] for i in idxTest ]) # test data \n",
    "            self.fit(X_learn,T_learn)                       # train regressifier\n",
    "            # (ii) test regressifier\n",
    "            for i in range(len(X_test)):  # loop over all data vectors to be tested\n",
    "                # (ii.a) regress for i-th test vector\n",
    "                xn_test = X_test[i].T                           # data vector for testing\n",
    "                t_test = self.predict(xn_test)                  # predict target value for given test vector \n",
    "                # (ii.b) check for regression errors\n",
    "                t_true = T_test[i].T                            # true target value\n",
    "                d=dist(t_test-t_true)                           # (Euklidean) distance between t_test and t_true \n",
    "                dttrue=dist(t_true)                             # length of t_true\n",
    "                E_dist  = E_dist+d                              # sum up distances (for first moment)\n",
    "                E_dist2 = E_dist2+d*d                           # sum up squared distances (for second moment)\n",
    "                if(E_max<0)or(d>E_max): E_max=d                 # collect maximal error\n",
    "                if(E_min<0)or(d<E_min): E_min=d                 # collect minimal error\n",
    "                drel=d/dttrue\n",
    "                Erel_dist  = Erel_dist+drel                     # sum up relative distances (for first moment)\n",
    "                Erel_dist2 = Erel_dist2+(drel*drel)             # sum up squared relative distances (for second moment)\n",
    "                if(Erel_max<0)or(drel>Erel_max): Erel_max=drel  # collect maximal relative error\n",
    "                if(Erel_min<0)or(drel<Erel_min): Erel_min=drel  # collect minimal relative error\n",
    "        E_dist      = E_dist/float(N)                           # estimate of first moment (expected error)\n",
    "        E_dist2     = E_dist2/float(N)                          # estimate of second moment (expected squared error)\n",
    "        Var_dist    = E_dist2-E_dist*E_dist                     # variance of error\n",
    "        sd_dist     = np.sqrt(Var_dist)                         # standard deviation of error\n",
    "        Erel_dist   = Erel_dist/float(N)                        # estimate of first moment (expected error)\n",
    "        Erel_dist2  = Erel_dist2/float(N)                       # estimate of second moment (expected squared error)\n",
    "        Varrel_dist = Erel_dist2-Erel_dist*Erel_dist            # variance of error\n",
    "        sdrel_dist  = np.sqrt(Varrel_dist)                      # standard deviation of error\n",
    "        return (E_dist,sd_dist,E_min,E_max), (Erel_dist,sdrel_dist,Erel_min,Erel_max) # return mean, standard deviation, minimum, \n",
    "                                                                # and maximum error (for absolute and relative distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------- \n",
    "# DataScaler: scale data to standardize data distribution (for mean=0, standard deviation =1)  \n",
    "# -------------------------------------------------------------------------------------------- \n",
    "class DataScaler: \n",
    "    \"\"\"\n",
    "    Class for standardizing data vectors \n",
    "    Some regression methods require standardizing of data before training to avoid numerical instabilities!!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,X):               # X is data matrix, where rows are data vectors\n",
    "        \"\"\"\n",
    "        Constructor: Set parameters (mean, std,...) to standardize data matrix X\n",
    "        :param X: Data matrix of size NxD the standardization parameters (mean, std, ...) should be computed for \n",
    "        :returns: object of class DataScaler\n",
    "        \"\"\"\n",
    "        self.meanX = np.mean(X,0)       # mean values for each feature column\n",
    "        self.stdX  = np.std(X,0)        # standard deviation for each feature column \n",
    "        if isinstance(self.stdX,(list,tuple,np.ndarray)): \n",
    "            self.stdX[self.stdX==0]=1.0 # do not scale data with zero std (that is, constant features)\n",
    "        else:\n",
    "            if(self.stdX==0): self.stdX=1.0   # in case stdX is a scalar\n",
    "        self.stdXinv = 1.0/self.stdX    # inverse standard deviation\n",
    "\n",
    "\n",
    "    def scale(self,x):                  # scales data vector x to mean=0 and std=1\n",
    "        \"\"\"\n",
    "        scale data vector (or data matrix) x to mean=0 and s.d.=1 \n",
    "        :param x: data vector or data matrix  \n",
    "        :returns: scaled (standardized) data vector or data matrix \n",
    "        \"\"\"\n",
    "        return np.multiply(x-self.meanX,self.stdXinv)\n",
    "\n",
    "    def unscale(self,x):                # unscale data vector x to original distribution\n",
    "        \"\"\"\n",
    "        unscale data vector (or data matrix) x to original data ranges  \n",
    "        :param x: standardized data vector or data matrix  \n",
    "        :returns: unscaled data vector or data matrix \n",
    "        \"\"\"\n",
    "        return np.multiply(x,self.stdX)+self.meanX\n",
    "\n",
    "    def printState(self):\n",
    "        \"\"\"\n",
    "        print standardization parameters (mean value, standard deviation (std), and inverse of std)  \n",
    "        \"\"\"\n",
    "        print(\"mean=\",self.meanX, \" std=\",self.stdX, \" std_inv=\",self.stdXinv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------- \n",
    "# function to compute polynomial basis functions \n",
    "# ----------------------------------------------------------------------------------------- \n",
    "def phi_polynomial(x,deg=1):           # x should be list or np.array or 1xD matrix; returns an 1xM matrix \n",
    "    \"\"\"\n",
    "    polynomial basis function vector; may be used to transform a data vector x into a feature vector phi(x) having polynomial basis function components\n",
    "    :param x: data vector to be transformed into a feature vector\n",
    "    :param deg: degree of polynomial\n",
    "    :returns phi: feature vector \n",
    "    Example: phi_polynomial(x,3) returns for one-dimensional x the vector [1, x, x*x, x*x*x]\n",
    "    \"\"\"\n",
    "    x=np.array(np.mat(x))[0]           # ensure that x is a 1D array (first row of x)\n",
    "    D=len(x)\n",
    "    assert (D==1) or ((D>1) and (deg<=3)), \"phi_polynomial(x,deg) not implemented for D=\"+str(D)+\" and deg=\"+str(deg)    # MODIFY CODE HERE FOR deg>3 !!!!\n",
    "    if(D==1):\n",
    "        phi = np.array([x[0]**i for i in range(deg+1)])\n",
    "    else:\n",
    "        phi = np.array([])\n",
    "        if(deg>=0):\n",
    "            phi = np.concatenate((phi,[1]))      # include degree 0 terms\n",
    "            if(deg>=1): \n",
    "                phi = np.concatenate((phi,x))    # includes degree 1 terms\n",
    "                if(deg>=2):\n",
    "                    for i in range(D):\n",
    "                        phi = np.concatenate(( phi, [x[i]*x[j] for j in range(i+1)] ))    # include degree 2 terms\n",
    "                    if(deg>=3):\n",
    "                        for i in range(D):\n",
    "                            for j in range(i+1):\n",
    "                                phi = np.concatenate(( phi, [x[i]*x[j]*x[k] for k in range(j+1)] ))   # include degree 3 terms\n",
    "                        # EXTEND CODE HERE FOR deg>3!!!!\n",
    "    return phi.T  # return basis function vector (=feature vector corresponding to data vector x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------\n",
    "# Least Squares (ML) linear regression with sum of squares Regularization,\n",
    "# -----------------------------------------------------------------------------------------\n",
    "class LSRRegressifier(Regressifier):\n",
    "    \"\"\"\n",
    "    Class for Least Squares (or Maximum Likelihood) Linear Regressifier with sum of squares regularization \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,lmbda=0,phi=lambda x: phi_polynomial(x,1),flagSTD=0,eps=1e-6):\n",
    "        \"\"\"\n",
    "        Constructor of class LSRegressifier\n",
    "        :param lmbda: Regularization coefficient lambda\n",
    "        :param phi: Basis-functions used by the linear model (default linear polynomial)\n",
    "        :param flagSTD: If >0 then standardize data X and target values T (to mean 0 and s.d. 1)\n",
    "        :param eps: maximal residual value to tolerate (instead of zero) for numerically good conditioned problems\n",
    "        :returns: -\n",
    "        \"\"\"\n",
    "        self.lmbda=lmbda       # set regression parameter (default 0)\n",
    "        self.phi=phi           # set basis functions used for linear regression (default: degree 1 polynomials)\n",
    "        self.flagSTD=flagSTD;  # if flag >0 then data will be standardized, i.e., scaled for mean 0 and s.d. 1\n",
    "        self.eps=eps;          # maximal residual value to tolerate (instead of zero) for numerically good conditioned problems\n",
    "\n",
    "\n",
    "    def fit(self,X,T,lmbda=None,phi=None,flagSTD=None): # train/compute LS regression with data matrix X and target value matrix T\n",
    "        \"\"\"\n",
    "        Train regressifier (see lecture manuscript, theorem 3.11, p33) \n",
    "        :param X: Data matrix of size NxD, contains in each row a data vector of size D\n",
    "        :param T: Target vector matrix of size NxK, contains in each row a target vector of size K\n",
    "        :param lmbda: Regularization coefficient lambda\n",
    "        :param phi: Basis-functions used by the linear model (default linear polynomial)\n",
    "        :param flagSTD: If >0 then standardize data X and target values T (to mean 0 and s.d. 1)\n",
    "        :returns: flagOK: if >0 then all is ok, otherwise matrix inversion was bad conditioned (and results should not be trusted!!!) \n",
    "        \"\"\"\n",
    "        # (i) set parameters\n",
    "        if lmbda==None: lmbda=self.lmbda       # reset regularization coefficient?\n",
    "        if phi==None: phi=self.phi             # reset basis functions?\n",
    "        if flagSTD==None: flagSTD=self.flagSTD # standardize data vectors?\n",
    "        # (ii) scale data for mean=0 and s.d.=0 ?\n",
    "        if flagSTD>0:                          # if yes, then...\n",
    "            self.datascalerX=DataScaler(X)     # create datascaler for data matrix X\n",
    "            self.datascalerT=DataScaler(T)     # create datascaler for target matrix T\n",
    "            X=self.datascalerX.scale(X)        # scale all features (=columns) of data matrix X to mean=0 and s.d.=1\n",
    "            T=self.datascalerT.scale(T)        # ditto for target matrix T\n",
    "        # (iii) compute weight matrix and check numerical condition\n",
    "        flagOK,maxZ=1,0;                       # if <1 then matrix inversion is numerically infeasible\n",
    "        try:\n",
    "            self.N,self.D = X.shape            # data matrix X has size N x D (N is number of data vectors, D is dimension of a vector)\n",
    "            self.M = self.phi(self.D*[0]).size # get number of basis functions  \n",
    "            self.K = T.shape[1]                # DELTE dummy code (just required for dummy code in predict(.): number of output dimensions\n",
    "            PHI = None                         # REPLACE dummy code: compute design matrix\n",
    "            PHIT_PHI_lmbdaI = None             # REPLACE dummy code: compute PHI_T*PHI+lambda*I\n",
    "            PHIT_PHI_lmbdaI_inv = None         # REPLACE dummy code: compute inverse matrix (may be bad conditioned and fail)\n",
    "            self.W_LSR = None                  # REPLACE dummy code: compute regularized least squares weights \n",
    "            # (iv) check numerical condition\n",
    "            Z=None                             # REPLACE dummy code: Compute Z:=PHIT_PHI_lmbdaI*PHIT_PHI_lmbdaI_inv-I which should become the zero matrix if good conditioned!\n",
    "            maxZ = 0                           # REPLACE dummy code: Compute maximum (absolute) componente of matrix Z (should be <eps for good conditioned problem)\n",
    "            assert maxZ<=self.eps              # maxZ should be <eps for good conditioned problems (otherwise the result cannot be trusted!!!)\n",
    "        except: \n",
    "            flagOK=0;\n",
    "            print(\"EXCEPTION DUE TO BAD CONDITION:flagOK=\", flagOK, \" maxZ=\", maxZ)\n",
    "            raise\n",
    "        return flagOK \n",
    "\n",
    "    def predict(self,x,flagSTD=None):      # predict a target value given data vector x \n",
    "        \"\"\"\n",
    "        predicts the target value y(x) for a test vector x\n",
    "        :param x: test data vector of size D\n",
    "        :param flagSTD: If >0 then standardize data X and target values T (to mean 0 and s.d. 1)\n",
    "        :returns: predicted target vector y of size K\n",
    "        \"\"\"\n",
    "        if flagSTD==None: flagSTD=self.flagSTD      # standardazion?\n",
    "        if flagSTD>0: x=self.datascalerX.scale(x)   # if yes, then scale x before computing the prediction!\n",
    "        phi_of_x = self.phi(x)                      # compute feature vector phi_of_x for data vector x\n",
    "        y=np.zeros((1,self.K)).T                    # REPLACE dummy code:  compute prediction y for data vector x \n",
    "        if flagSTD>0: y=self.datascalerT.unscale(y) # scale prediction back to original range?\n",
    "        return y                  # return prediction y for data vector x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------\n",
    "# KNN regression \n",
    "# -----------------------------------------------------------------------------------------\n",
    "class KNNRegressifier(Regressifier): \n",
    "    \"\"\"\n",
    "    Class for fast K-Nearest-Neighbor-Regression using KD-trees \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,K,flagKLinReg=0):\n",
    "        \"\"\"\n",
    "        Constructor of class KNNRegressifier\n",
    "        :param K: number of nearest neighbors that are used to compute prediction \n",
    "        :flagKLinReg: if >0 then the do a linear (least squares) regression on the the K nearest neighbors and their target values\n",
    "                      otherwise just take the mean of the K nearest neighbors target vectors\n",
    "        :returns: -\n",
    "        \"\"\"\n",
    "        self.K = K                                 # K is number of nearest-neighbors used for majority decision\n",
    "        self.X, self.T = [],[]                     # initially no data is stored\n",
    "        self.flagKLinReg=flagKLinReg               # if flag is set then do a linear regression of the KNN (otherwise just return mean T of the KNN)\n",
    "\n",
    "    def fit(self,X,T): # train/compute regression with lists of data vectors X and target values T\n",
    "        \"\"\"\n",
    "        Train regressifier by stroing X and T and by creating a KD-Tree based on X   \n",
    "        :param X: Data matrix of size NxD, contains in each row a data vector of size D\n",
    "        :param T: Target vector matrix of size NxK, contains in each row a target vector of size K\n",
    "        :returns: -\n",
    "        \"\"\"\n",
    "        self.X, self.T = np.array(X),np.array(T)   # just store feature vectors X and corresponding class labels T\n",
    "        self.N, self.D = self.X.shape              # store data number N and dimension D\n",
    "        self.kdtree = scipy.spatial.KDTree(self.X) # do an indexing of the feature vectors\n",
    "\n",
    "    def predict(self,x,K=None,flagKLinReg=None):   # predict a target value given data vector x \n",
    "        \"\"\"\n",
    "        predicts the target value y(x) for a test vector x\n",
    "        :param x: test data vector of size D\n",
    "        :param K: number of nearest neighbors that are used to compute prediction \n",
    "        :flagKLinReg: if >0 then the do a linear (least squares) regression on the the K nearest neighbors and their target values\n",
    "                      otherwise just take the mean of the K nearest neighbors target vectors\n",
    "        :returns: predicted target vector of size K\n",
    "        \"\"\"\n",
    "        if(K==None): K=self.K                      # do a K-NN search...\n",
    "        if(flagKLinReg==None): flagKLinReg=self.flagKLinReg # if flag >0 then do a regression on the K Nearest Neighbors to get the prediction\n",
    "        nn = self.kdtree.query(x,K)                # get indexes of K nearest neighbors of x\n",
    "        if K==1: idxNN=[nn[1]]                     # cast nearest neighbor indexes nn as a list idxNN\n",
    "        else: idxNN=nn[1]\n",
    "        t_out=0\n",
    "        if(self.flagKLinReg==0):\n",
    "            # just take mean value of KNNs\n",
    "            t_out=np.mean([self.T[i] for i in idxNN])\n",
    "        else:\n",
    "            # do a linear regression of the KNNs\n",
    "            lsr=LSRegressifier(lmbda=0.0001,phi=lambda x:phi_polynomial(x,1),flagSTD=1)\n",
    "            lsr.fit(self.X[idxNN],self.T[idxNN])\n",
    "            t_out=lsr.predict(x)\n",
    "        return t_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------\n",
      "Example: 1D-linear regression problem\n",
      "-----------------------------------------\n",
      "X= [[ 0. ]\n",
      " [ 0.5]\n",
      " [ 1. ]\n",
      " [ 1.5]\n",
      " [ 2. ]\n",
      " [ 2.5]\n",
      " [ 3. ]\n",
      " [ 3.5]\n",
      " [ 4. ]\n",
      " [ 4.5]\n",
      " [ 5. ]\n",
      " [ 5.5]\n",
      " [ 6. ]\n",
      " [ 6.5]\n",
      " [ 7. ]\n",
      " [ 7.5]\n",
      " [ 8. ]\n",
      " [ 8.5]\n",
      " [ 9. ]\n",
      " [ 9.5]\n",
      " [10. ]\n",
      " [10.5]\n",
      " [11. ]\n",
      " [11.5]\n",
      " [12. ]\n",
      " [12.5]\n",
      " [13. ]\n",
      " [13.5]\n",
      " [14. ]\n",
      " [14.5]\n",
      " [15. ]\n",
      " [15.5]\n",
      " [16. ]\n",
      " [16.5]\n",
      " [17. ]\n",
      " [17.5]\n",
      " [18. ]\n",
      " [18.5]\n",
      " [19. ]\n",
      " [19.5]\n",
      " [20. ]\n",
      " [20.5]\n",
      " [21. ]\n",
      " [21.5]\n",
      " [22. ]\n",
      " [22.5]\n",
      " [23. ]\n",
      " [23.5]\n",
      " [24. ]\n",
      " [24.5]\n",
      " [25. ]\n",
      " [25.5]\n",
      " [26. ]\n",
      " [26.5]\n",
      " [27. ]\n",
      " [27.5]\n",
      " [28. ]\n",
      " [28.5]\n",
      " [29. ]\n",
      " [29.5]\n",
      " [30. ]\n",
      " [30.5]\n",
      " [31. ]\n",
      " [31.5]\n",
      " [32. ]\n",
      " [32.5]\n",
      " [33. ]\n",
      " [33.5]\n",
      " [34. ]\n",
      " [34.5]\n",
      " [35. ]\n",
      " [35.5]\n",
      " [36. ]\n",
      " [36.5]\n",
      " [37. ]\n",
      " [37.5]\n",
      " [38. ]\n",
      " [38.5]\n",
      " [39. ]\n",
      " [39.5]\n",
      " [40. ]\n",
      " [40.5]\n",
      " [41. ]\n",
      " [41.5]\n",
      " [42. ]\n",
      " [42.5]\n",
      " [43. ]\n",
      " [43.5]\n",
      " [44. ]\n",
      " [44.5]\n",
      " [45. ]\n",
      " [45.5]\n",
      " [46. ]\n",
      " [46.5]\n",
      " [47. ]\n",
      " [47.5]\n",
      " [48. ]\n",
      " [48.5]\n",
      " [49. ]\n",
      " [49.5]]\n",
      "T= [[  3.41839088]\n",
      " [  5.96720741]\n",
      " [  7.59136156]\n",
      " [  6.7710373 ]\n",
      " [  7.73897824]\n",
      " [  9.64611992]\n",
      " [ 10.96592621]\n",
      " [ 11.11718615]\n",
      " [  9.30737657]\n",
      " [ 12.94584495]\n",
      " [ 14.42033653]\n",
      " [ 14.3547631 ]\n",
      " [ 15.44076294]\n",
      " [ 18.56852147]\n",
      " [ 18.35677963]\n",
      " [ 18.92276915]\n",
      " [ 21.98092739]\n",
      " [ 19.52303825]\n",
      " [ 22.5686096 ]\n",
      " [ 22.53503383]\n",
      " [ 23.43716785]\n",
      " [ 24.42824122]\n",
      " [ 26.44467738]\n",
      " [ 25.79355958]\n",
      " [ 27.96195909]\n",
      " [ 30.09866179]\n",
      " [ 28.02124967]\n",
      " [ 31.09072275]\n",
      " [ 32.13128259]\n",
      " [ 34.84705774]\n",
      " [ 32.60237194]\n",
      " [ 35.07327132]\n",
      " [ 35.39770274]\n",
      " [ 38.80732612]\n",
      " [ 39.32188252]\n",
      " [ 38.36193688]\n",
      " [ 39.36217461]\n",
      " [ 39.53968369]\n",
      " [ 40.96453218]\n",
      " [ 43.52068501]\n",
      " [ 45.02465439]\n",
      " [ 45.37371008]\n",
      " [ 47.39260055]\n",
      " [ 48.24977226]\n",
      " [ 45.77011536]\n",
      " [ 46.82285955]\n",
      " [ 49.76977381]\n",
      " [ 53.05672296]\n",
      " [ 52.40478263]\n",
      " [ 53.387185  ]\n",
      " [ 55.1948312 ]\n",
      " [ 57.64082308]\n",
      " [ 55.08763038]\n",
      " [ 58.32392937]\n",
      " [ 59.37034486]\n",
      " [ 58.60705812]\n",
      " [ 59.19088592]\n",
      " [ 62.01593921]\n",
      " [ 60.83995921]\n",
      " [ 63.40065986]\n",
      " [ 64.33168306]\n",
      " [ 65.35440323]\n",
      " [ 65.57874098]\n",
      " [ 67.47603063]\n",
      " [ 67.66096485]\n",
      " [ 69.07641477]\n",
      " [ 71.04320281]\n",
      " [ 69.81758952]\n",
      " [ 71.64417302]\n",
      " [ 73.77193814]\n",
      " [ 74.10208131]\n",
      " [ 75.57414348]\n",
      " [ 74.72896192]\n",
      " [ 78.77685266]\n",
      " [ 78.26663398]\n",
      " [ 76.71760559]\n",
      " [ 81.19662711]\n",
      " [ 82.5679989 ]\n",
      " [ 81.19213467]\n",
      " [ 81.81518384]\n",
      " [ 82.67215929]\n",
      " [ 83.58700158]\n",
      " [ 86.25882098]\n",
      " [ 86.68105329]\n",
      " [ 87.47776467]\n",
      " [ 88.02731956]\n",
      " [ 90.20160502]\n",
      " [ 91.23051702]\n",
      " [ 92.127954  ]\n",
      " [ 93.99546831]\n",
      " [ 94.23596133]\n",
      " [ 95.53372126]\n",
      " [ 96.39169233]\n",
      " [ 97.04250992]\n",
      " [ 98.83895796]\n",
      " [ 99.43265551]\n",
      " [100.45265104]\n",
      " [100.98213946]\n",
      " [102.52785358]\n",
      " [102.24817576]]\n",
      "phi(4)= [ 1  4 16]\n",
      "phi([1,2])= [1. 1. 2. 1. 2. 4.]\n",
      "\n",
      "-----------------------------------------\n",
      "Do a Least-Squares-Regression\n",
      "-----------------------------------------\n",
      "lsr.W_LSR= None\n",
      "prediction of x= [3.1415] is y= [[0.]]\n",
      "LSRRegression cross-validation: absolute errors (E,sd,min,max)= (53.56910701881888, 28.884121890928842, 3.4183908788491437, 102.52785358064992)   relative errors (E,sd,min,max)= (1.0, 0.0, 1.0, 1.0)\n",
      "\n",
      "-----------------------------------------\n",
      "Do a KNN-Regression\n",
      "-----------------------------------------\n",
      "prediction of x= [3.1415] is y= 9.755117420146817\n",
      "KNNRegression cross-validation: absolute errors (E,sd,min,max)= (1.196763284873125, 0.9911180834126481, 0.011879280296369643, 5.475104520487159)   relative errors (E,sd,min,max)= (0.050722381744736623, 0.1637800819566731, 0.00017014738517792856, 1.6016613414117349)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(\"\\n-----------------------------------------\")\n",
    "    print(\"Example: 1D-linear regression problem\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    # (i) generate data\n",
    "    N=100\n",
    "    w0,w1=4,2                 # parameters of line\n",
    "    X=np.zeros((N,1))         # x data: allocate Nx1 matrix as numpy ndarray\n",
    "    X[:,0]=np.arange(0,50.0,50.0/N)  # equidistant sampling of the interval [0,50)\n",
    "    T=np.zeros((N,1))         # target values: allocate Nx1 matrix as numpy ndarray\n",
    "    sd_noise = 1.0            # noise power (=standard deviation)\n",
    "    T=T+w1*X+w0 + np.random.normal(0,sd_noise,T.shape)  # generate noisy target values on line y=w0+w1*x\n",
    "    par_lambda = 0            # regularization parameter\n",
    "    print(\"X=\",X)\n",
    "    print(\"T=\",T)\n",
    "\n",
    "    # (ii) define basis functions (phi should return list of basis functions; x should be a list)\n",
    "    deg=2;                               # degree of polynomial\n",
    "    phi=lambda x: phi_polynomial(x,2)    # define phi by polynomial basis-functions up to degree deg \n",
    "    print(\"phi(4)=\", phi([4]))            # print basis function vector [1, x, x*x ...] for x=4\n",
    "    print(\"phi([1,2])=\", phi([1,2]))      # print basis function vector for two-dim. inputs (yields many output components) \n",
    "\n",
    "    # (iii) compute LSR regression\n",
    "    print(\"\\n-----------------------------------------\")\n",
    "    print(\"Do a Least-Squares-Regression\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    lmbda=0;\n",
    "    lsr = LSRRegressifier(lmbda,phi)\n",
    "    lsr.fit(X,T)\n",
    "    print(\"lsr.W_LSR=\",lsr.W_LSR)           # weight vector (should be approximately [w0,w1]=[4,2])\n",
    "    x=np.array([3.1415]).T\n",
    "    print(\"prediction of x=\",x,\"is y=\",lsr.predict(x))\n",
    "\n",
    "    # do S-fold crossvalidation\n",
    "    S=3\n",
    "    err_abs,err_rel = lsr.crossvalidate(S,X,T)\n",
    "    print(\"LSRRegression cross-validation: absolute errors (E,sd,min,max)=\", err_abs, \"  relative errors (E,sd,min,max)=\", err_rel)\n",
    "\n",
    "    # (iv) compute KNN-regression\n",
    "    print(\"\\n-----------------------------------------\")\n",
    "    print(\"Do a KNN-Regression\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    K=5;\n",
    "    knnr = KNNRegressifier(K)\n",
    "    knnr.fit(X,T)\n",
    "    print(\"prediction of x=\",x,\"is y=\",knnr.predict(x))\n",
    "\n",
    "    # do S-fold crossvalidation\n",
    "    err_abs,err_rel = knnr.crossvalidate(S,X,T)\n",
    "    print(\"KNNRegression cross-validation: absolute errors (E,sd,min,max)=\", err_abs, \"  relative errors (E,sd,min,max)=\", err_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
