{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# increase display width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 2.2\n",
    "**Thema: Python-Modul für Lineare und k-Nearest-Neighbor (KNN) Regression** <br>\n",
    "Da wir im folgenden verschiedene Regressions-Verfahren implementieren wollen lohnt es sich\n",
    "ein eigenes Python-Modul dafür zu erstellen (siehe Programmgerüst V2A2_Regression.py).\n",
    "## a)\n",
    "Versuchen Sie zunächst den Aufbau des Moduls V2A2_Regression.py zu verstehen:\n",
    "   - Betrachten Sie den Aufbau des Moduls durch Eingabe von pydoc V2A2_Regressifier. Welche Klassen gehören zu dem Modul und welchen Zweck haben sie jeweils?\n",
    "       - Regressifier: Abstrakte Basis-Klasse für Regressifier\n",
    "       - DataScaler: Standartisiert die Datenvektoren\n",
    "       - LSRRegressifier: Regression mit Hilfe von Least-Square \n",
    "       - KNNRegressifier: Regression mit Hilfe von Kd-Trees\n",
    "   - Betrachten Sie nun die Basis-Klasse Regressifier im Quelltext: Wozu dienen jeweils die Methoden fit(self,X,T), predict(self,x) und crossvalidate(self,S,X,T)?\n",
    "       - fit(self,X,T): trainiert mit Datenvektoren X und den Labeln T\n",
    "       - predict(self,x): berechnet y(x)\n",
    "       - crossvalidate(self,S,X,T): führt eine Kreuzvalidierung durch. S = Anzahl der Datensets, X = Datenvektoren, T = Labels\n",
    "   - Worin unterscheidet sich crossvalidate(.) von der entsprechenden Methode für Klassifikation (siehe vorigen Versuch)?\n",
    "       - Die beiden Funktionen liefern unterschiedliche Ergebnisse, bei der Klassifikation die Fehlerwahrscheinlichkeiten und bei der Regression den Durchschnitt, die Normalverteilung, Minimum und Maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)\n",
    "Betrachten Sie nun die Funktion phi_polynomial(x,deg):\n",
    "   - Was berechnet die Funktion? Welches Ergebnis liefert phi_polynomial([3],5)? Welches Ergebnis liefert phi_polynomial([3,5],2)?\n",
    "       - berechnet das Polynom zur eingabe x mit dem Grad deg\n",
    "       - ([3],5): 1, 3, 9, 27, 81, 243\n",
    "       - ([3,5],2): 1, 3, 5, 9, 15, 25\n",
    "   - Geben Sie eine allgemeine Formel an für das Ergebnis von phi_polynomial([x1,x2],2)?\n",
    "       - $ (1,x_1,x_2,x_1^2,x_1*x_2,x_2^2)$\n",
    "   - Wozu braucht man diese Funktion im Zusammenhang mit Regression?\n",
    "       - wird zum lernen und zum predicten benötigt\n",
    "   - Bis zu welchem Polynomgrad kann die Funktion Basisfunktionen berechnen? Erweitern Sie die Funktion mindestens bis Grad 5.\n",
    "       - done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c)\n",
    "Betrachten Sie die Klasse LSRRegressifier:\n",
    "   - Welche Art von Regressions-Modell berechnet diese Klasse?\n",
    "       - Lineare Regression mit Least-Squares Regularisierung\n",
    "   - Wozu dienen jeweils die Parameter lmbda, phi, flagSTD und eps ?\n",
    "       - lmbda: Regularisierungs-Koeffizient $\\lambda$\n",
    "       - phi: Basisfunktion\n",
    "       - flagSTD: falls >0 werden die Daten standardisiert mit DataScaler\n",
    "       - eps: höchst zulässiger Fehlerwert\n",
    "   - Welche Rolle spielt hier die Klasse DataScaler? <br> In welchen Methoden und zu welchem Zweck werden die Daten ggf. umskaliert? <br>Welches Problem kann auftreten wenn man dies nicht tut? <br> Wozu braucht man die Variablen Z und maxZ in der Methode fit(.)?\n",
    "       - Die Methode skaliert ggf. die Daten\n",
    "       - in prediction() und fit(), da sonst die Gewichte falsch berechnet werden\n",
    "       - ???\n",
    "       - Z ist die Fehlerwertmatrix und maxZ gibt den größten Fehlerwert an. Dies wird benötigt um eps zu überprüfen und bei zu schlechter Konditionierung zu warnen.\n",
    "   - Vervollständigen Sie die Methoden fit(self,X,T,...) und predict(self,x,...) (vgl. vorige Aufgabe).\n",
    "       - done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d)\n",
    "Betrachten Sie die Klasse KNNRegressifier:\n",
    "   - Welche Art von Regressions-Modell berechnet diese Klasse?\n",
    "       - k-nearest-neigbor-regression\n",
    "   - Wozu dienen jeweils die Parameter K und flagKLinReg?\n",
    "       - K: Anzahl der nächsetn Nachbarn, die genutzt werden für die Berechnungen\n",
    "       - flagKLinReg: wenn >0 dann wird lineare Regression verwendet, sonst der Durchschnitt\n",
    "   - Beschreiben Sie kurz in eigenen Worten (2-3 Sätze) auf welche Weise die Prädiktion y(x) berechnet wird.\n",
    "       - ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e)\n",
    "Betrachten Sie abschließend den Modultest:\n",
    "   - Beschreiben Sie kurz was im Modultest passiert.\n",
    "   \n",
    "   - Welche Gewichte W werden gelernt? Wie lautet also die gelernte Prädiktionsfunktion? Welche Funktion sollte sich idealerweise (für N ! 1) ergeben?\n",
    "   \n",
    "   - Welche Ergebnisse liefert die Kreuzvalidierung? Was bedeuten die Werte?\n",
    "   \n",
    "   - Vergleichen und Bewerten Sie die Ergebnisse von Least Squares Regression gegenüber der KNN-Regression (nach Optimierung der Hyper-Parameter $\\lambda$, K, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.spatial\n",
    "from random import randint\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# supress the 1.0+e02\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------- \n",
    "# base class for regressifiers\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "class Regressifier:\n",
    "    \"\"\"\n",
    "    Abstract base class for regressifiers\n",
    "    Inherit from this class to implement a concrete regression algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self,X,T):        # train/compute regression with lists of feature vectors X and class labels T\n",
    "        \"\"\"\n",
    "        Train regressifier by training data X, T, should be overwritten by any derived class\n",
    "        :param X: Data matrix of size NxD, contains in each row a data vector of size D\n",
    "        :param T: Target vector matrix of size NxK, contains in each row a target vector of size K\n",
    "        :returns: -\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self,x):      # predict a target vector given the data vector x \n",
    "        \"\"\"\n",
    "        Implementation of the regression algorithm; should be overwritten by any derived class \n",
    "        :param x: test data vector of size D\n",
    "        :returns: predicted target vector\n",
    "        \"\"\"\n",
    "        return None           \n",
    "\n",
    "    def crossvalidate(self,S,X,T,dist=lambda t: np.linalg.norm(t)):  # do a S-fold cross validation \n",
    "        \"\"\"\n",
    "        Do a S-fold cross validation\n",
    "        :param S: Number of parts the data set is divided into\n",
    "        :param X: Data matrix (one data vector per row)\n",
    "        :param T: Matrix of target vectors; T[n] is target vector of X[n]\n",
    "        :param dist: a fuction dist(t) returning the length of vector t (default=Euklidean)\n",
    "        :returns (E_dist,sd_dist,E_min,E_max) : mean, standard deviation, minimum, and maximum of absolute error \n",
    "        :returns (Erel_dist,sdrel_dist,Erel_min,Erel_max) : mean, standard deviation, minimum, and maximum of relative error \n",
    "        \"\"\"\n",
    "        X,T=np.array(X),np.array(T)                         # ensure array type\n",
    "        N=len(X)                                            # N=number of data vectors\n",
    "        perm = np.random.permutation(N)                     # do a random permutation of X and T...\n",
    "        X1,T1=[X[i] for i in perm], [T[i] for i in perm]    # ... to get random partitions of the data set\n",
    "        idxS = [range(i*N//S,(i+1)*N//S) for i in range(S)] # divide data set into S parts:\n",
    "        E_dist,E_dist2,E_max,E_min=0,0,-1,-1                # initialize first two moments of (absolute) regression error as well as max/min error \n",
    "        Erel_dist,Erel_dist2,Erel_max,Erel_min=0,0,-1,-1    # initialize first two moments of relative regression error as well as max/min error \n",
    "        for idxTest in idxS:                                # loop over all possible test data sets\n",
    "            # (i) generate training and testing data sets and train classifier        \n",
    "            idxLearn = [i for i in range(N) if i not in idxTest]                      # remaining indices (not in idxTest) are learning data\n",
    "            if(S<=1): idxLearn=idxTest                                                # if S==1 use entire data set for learning and testing\n",
    "            X_learn, T_learn = np.array([X1[i] for i in idxLearn]), np.array([T1[i] for i in idxLearn]) # learn data \n",
    "            X_test , T_test  = np.array([X1[i] for i in idxTest ]), np.array([T1[i] for i in idxTest ]) # test data \n",
    "            self.fit(X_learn,T_learn)                       # train regressifier\n",
    "            # (ii) test regressifier\n",
    "            for i in range(len(X_test)):  # loop over all data vectors to be tested\n",
    "                # (ii.a) regress for i-th test vector\n",
    "                xn_test = X_test[i].T                           # data vector for testing\n",
    "                t_test = self.predict(xn_test)                  # predict target value for given test vector \n",
    "                # (ii.b) check for regression errors\n",
    "                t_true = T_test[i].T                            # true target value\n",
    "                d=dist(t_test-t_true)                           # (Euklidean) distance between t_test and t_true \n",
    "                dttrue=dist(t_true)                             # length of t_true\n",
    "                E_dist  = E_dist+d                              # sum up distances (for first moment)\n",
    "                E_dist2 = E_dist2+d*d                           # sum up squared distances (for second moment)\n",
    "                if(E_max<0)or(d>E_max): E_max=d                 # collect maximal error\n",
    "                if(E_min<0)or(d<E_min): E_min=d                 # collect minimal error\n",
    "                drel=d/dttrue\n",
    "                Erel_dist  = Erel_dist+drel                     # sum up relative distances (for first moment)\n",
    "                Erel_dist2 = Erel_dist2+(drel*drel)             # sum up squared relative distances (for second moment)\n",
    "                if(Erel_max<0)or(drel>Erel_max): Erel_max=drel  # collect maximal relative error\n",
    "                if(Erel_min<0)or(drel<Erel_min): Erel_min=drel  # collect minimal relative error\n",
    "        E_dist      = E_dist/float(N)                           # estimate of first moment (expected error)\n",
    "        E_dist2     = E_dist2/float(N)                          # estimate of second moment (expected squared error)\n",
    "        Var_dist    = E_dist2-E_dist*E_dist                     # variance of error\n",
    "        sd_dist     = np.sqrt(Var_dist)                         # standard deviation of error\n",
    "        Erel_dist   = Erel_dist/float(N)                        # estimate of first moment (expected error)\n",
    "        Erel_dist2  = Erel_dist2/float(N)                       # estimate of second moment (expected squared error)\n",
    "        Varrel_dist = Erel_dist2-Erel_dist*Erel_dist            # variance of error\n",
    "        sdrel_dist  = np.sqrt(Varrel_dist)                      # standard deviation of error\n",
    "        return (E_dist,sd_dist,E_min,E_max), (Erel_dist,sdrel_dist,Erel_min,Erel_max) # return mean, standard deviation, minimum, \n",
    "                                                                # and maximum error (for absolute and relative distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------- \n",
    "# DataScaler: scale data to standardize data distribution (for mean=0, standard deviation =1)  \n",
    "# -------------------------------------------------------------------------------------------- \n",
    "class DataScaler: \n",
    "    \"\"\"\n",
    "    Class for standardizing data vectors \n",
    "    Some regression methods require standardizing of data before training to avoid numerical instabilities!!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,X):               # X is data matrix, where rows are data vectors\n",
    "        \"\"\"\n",
    "        Constructor: Set parameters (mean, std,...) to standardize data matrix X\n",
    "        :param X: Data matrix of size NxD the standardization parameters (mean, std, ...) should be computed for \n",
    "        :returns: object of class DataScaler\n",
    "        \"\"\"\n",
    "        self.meanX = np.mean(X,0)       # mean values for each feature column\n",
    "        self.stdX  = np.std(X,0)        # standard deviation for each feature column \n",
    "        if isinstance(self.stdX,(list,tuple,np.ndarray)): \n",
    "            self.stdX[self.stdX==0]=1.0 # do not scale data with zero std (that is, constant features)\n",
    "        else:\n",
    "            if(self.stdX==0): self.stdX=1.0   # in case stdX is a scalar\n",
    "        self.stdXinv = 1.0/self.stdX    # inverse standard deviation\n",
    "\n",
    "\n",
    "    def scale(self,x):                  # scales data vector x to mean=0 and std=1\n",
    "        \"\"\"\n",
    "        scale data vector (or data matrix) x to mean=0 and s.d.=1 \n",
    "        :param x: data vector or data matrix  \n",
    "        :returns: scaled (standardized) data vector or data matrix \n",
    "        \"\"\"\n",
    "        return np.multiply(x-self.meanX,self.stdXinv)\n",
    "\n",
    "    def unscale(self,x):                # unscale data vector x to original distribution\n",
    "        \"\"\"\n",
    "        unscale data vector (or data matrix) x to original data ranges  \n",
    "        :param x: standardized data vector or data matrix  \n",
    "        :returns: unscaled data vector or data matrix \n",
    "        \"\"\"\n",
    "        return np.multiply(x,self.stdX)+self.meanX\n",
    "\n",
    "    def printState(self):\n",
    "        \"\"\"\n",
    "        print standardization parameters (mean value, standard deviation (std), and inverse of std)  \n",
    "        \"\"\"\n",
    "        print(\"mean=\",self.meanX, \" std=\",self.stdX, \" std_inv=\",self.stdXinv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------- \n",
    "# function to compute polynomial basis functions \n",
    "# ----------------------------------------------------------------------------------------- \n",
    "def phi_polynomial(x,deg=1):           # x should be list or np.array or 1xD matrix; returns an 1xM matrix \n",
    "    \"\"\"\n",
    "    polynomial basis function vector; may be used to transform a data vector x into a feature vector phi(x) having polynomial basis function components\n",
    "    :param x: data vector to be transformed into a feature vector\n",
    "    :param deg: degree of polynomial\n",
    "    :returns phi: feature vector \n",
    "    Example: phi_polynomial(x,3) returns for one-dimensional x the vector [1, x, x*x, x*x*x]\n",
    "    \"\"\"\n",
    "    x=np.array(np.mat(x))[0]           # ensure that x is a 1D array (first row of x)\n",
    "    D=len(x)\n",
    "    #assert (D==1) or ((D>1) and (deg<=3)), \"phi_polynomial(x,deg) not implemented for D=\"+str(D)+\" and deg=\"+str(deg)    # MODIFY CODE HERE FOR deg>3 !!!!\n",
    "    if(D==1):\n",
    "        phi = np.array([x[0]**i for i in range(deg+1)])\n",
    "    else:\n",
    "        phi = np.array([])\n",
    "        if(deg>=0):\n",
    "            phi = np.concatenate((phi,[1]))      # include degree 0 terms\n",
    "            if(deg>=1): \n",
    "                phi = np.concatenate((phi,x))    # includes degree 1 terms\n",
    "                if(deg>=2):\n",
    "                    for i in range(D):\n",
    "                        phi = np.concatenate(( phi, [x[i]*x[j] for j in range(i+1)] ))    # include degree 2 terms\n",
    "                    if(deg>=3):\n",
    "                        for i in range(D):\n",
    "                            for j in range(i+1):\n",
    "                                phi = np.concatenate(( phi, [x[i]*x[j]*x[k] for k in range(j+1)] ))   # include degree 3 terms\n",
    "                        if(deg>=4):\n",
    "                            X=[x]\n",
    "                            poly = PolynomialFeatures(degree=deg)\n",
    "                            phi= poly.fit_transform(X)[0]\n",
    "    return phi.T  # return basis function vector (=feature vector corresponding to data vector x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------\n",
    "# Least Squares (ML) linear regression with sum of squares Regularization,\n",
    "# -----------------------------------------------------------------------------------------\n",
    "class LSRRegressifier(Regressifier):\n",
    "    \"\"\"\n",
    "    Class for Least Squares (or Maximum Likelihood) Linear Regressifier with sum of squares regularization \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,lmbda=0,phi=lambda x: phi_polynomial(x,1),flagSTD=0,eps=1e-6):\n",
    "        \"\"\"\n",
    "        Constructor of class LSRegressifier\n",
    "        :param lmbda: Regularization coefficient lambda\n",
    "        :param phi: Basis-functions used by the linear model (default linear polynomial)\n",
    "        :param flagSTD: If >0 then standardize data X and target values T (to mean 0 and s.d. 1)\n",
    "        :param eps: maximal residual value to tolerate (instead of zero) for numerically good conditioned problems\n",
    "        :returns: -\n",
    "        \"\"\"\n",
    "        self.lmbda=lmbda       # set regression parameter (default 0)\n",
    "        self.phi=phi           # set basis functions used for linear regression (default: degree 1 polynomials)\n",
    "        self.flagSTD=flagSTD;  # if flag >0 then data will be standardized, i.e., scaled for mean 0 and s.d. 1\n",
    "        self.eps=eps;          # maximal residual value to tolerate (instead of zero) for numerically good conditioned problems\n",
    "\n",
    "\n",
    "    def fit(self,X,T,lmbda=None,phi=None,flagSTD=None): # train/compute LS regression with data matrix X and target value matrix T\n",
    "        \"\"\"\n",
    "        Train regressifier (see lecture manuscript, theorem 3.11, p33) \n",
    "        :param X: Data matrix of size NxD, contains in each row a data vector of size D\n",
    "        :param T: Target vector matrix of size NxK, contains in each row a target vector of size K\n",
    "        :param lmbda: Regularization coefficient lambda\n",
    "        :param phi: Basis-functions used by the linear model (default linear polynomial)\n",
    "        :param flagSTD: If >0 then standardize data X and target values T (to mean 0 and s.d. 1)\n",
    "        :returns: flagOK: if >0 then all is ok, otherwise matrix inversion was bad conditioned (and results should not be trusted!!!) \n",
    "        \"\"\"\n",
    "        # (i) set parameters\n",
    "        if lmbda==None: lmbda=self.lmbda       # reset regularization coefficient?\n",
    "        if phi==None: phi=self.phi             # reset basis functions?\n",
    "        if flagSTD==None: flagSTD=self.flagSTD # standardize data vectors?\n",
    "        # (ii) scale data for mean=0 and s.d.=0 ?\n",
    "        if flagSTD>0:                          # if yes, then...\n",
    "            self.datascalerX=DataScaler(X)     # create datascaler for data matrix X\n",
    "            self.datascalerT=DataScaler(T)     # create datascaler for target matrix T\n",
    "            X=self.datascalerX.scale(X)        # scale all features (=columns) of data matrix X to mean=0 and s.d.=1\n",
    "            T=self.datascalerT.scale(T)        # ditto for target matrix T\n",
    "        # (iii) compute weight matrix and check numerical condition\n",
    "        flagOK,maxZ=1,0;                       # if <1 then matrix inversion is numerically infeasible\n",
    "        try:\n",
    "            self.N,self.D = X.shape            # data matrix X has size N x D (N is number of data vectors, D is dimension of a vector)\n",
    "            self.M = self.phi(self.D*[0]).size # get number of basis functions  \n",
    "            #self.K = T.shape[1]                # DELTE dummy code (just required for dummy code in predict(.): number of output dimensions\n",
    "            #PHI = None                         # REPLACE dummy code: compute design matrix\n",
    "            PHI = np.array([phi(X[i]).T for i in range(self.N)])\n",
    "            #PHIT_PHI_lmbdaI = None             # REPLACE dummy code: compute PHI_T*PHI+lambda*I\n",
    "            PHIT_PHI_lmbdaI = np.add(PHI.T.dot(PHI),np.dot(self.lmbda,np.eye(self.M)))\n",
    "            #PHIT_PHI_lmbdaI_inv = None         # REPLACE dummy code: compute inverse matrix (may be bad conditioned and fail)\n",
    "            PHIT_PHI_lmbdaI_inv = np.linalg.inv(PHIT_PHI_lmbdaI)\n",
    "            #self.W_LSR = None                  # REPLACE dummy code: compute regularized least squares weights \n",
    "            self.W_LSR = np.linalg.lstsq(PHI.T.dot(PHI) + np.dot(lmbda,np.eye(self.M)), PHI.T.dot(T))[0]\n",
    "            # (iv) check numerical condition\n",
    "            #Z=None                             # REPLACE dummy code: Compute Z:=PHIT_PHI_lmbdaI*PHIT_PHI_lmbdaI_inv-I which should become the zero matrix if good conditioned!\n",
    "            Z = np.dot(PHIT_PHI_lmbdaI,PHIT_PHI_lmbdaI_inv)-np.eye(self.M)\n",
    "            maxZ = np.max(Z)                    # REPLACE dummy code: Compute maximum (absolute) componente of matrix Z (should be <eps for good conditioned problem)\n",
    "            assert maxZ<=self.eps               # maxZ should be <eps for good conditioned problems (otherwise the result cannot be trusted!!!)\n",
    "        except: \n",
    "            flagOK=0;\n",
    "            print(\"EXCEPTION DUE TO BAD CONDITION:flagOK=\", flagOK, \" maxZ=\", maxZ)\n",
    "            raise\n",
    "        return flagOK \n",
    "\n",
    "    def predict(self,x,flagSTD=None):      # predict a target value given data vector x \n",
    "        \"\"\"\n",
    "        predicts the target value y(x) for a test vector x\n",
    "        :param x: test data vector of size D\n",
    "        :param flagSTD: If >0 then standardize data X and target values T (to mean 0 and s.d. 1)\n",
    "        :returns: predicted target vector y of size K\n",
    "        \"\"\"\n",
    "        if flagSTD==None: flagSTD=self.flagSTD      # standardazion?\n",
    "        if flagSTD>0: x=self.datascalerX.scale(x)   # if yes, then scale x before computing the prediction!\n",
    "        phi_of_x = self.phi(x)                      # compute feature vector phi_of_x for data vector x\n",
    "        #y=np.zeros((1,self.K)).T                    # REPLACE dummy code:  compute prediction y for data vector x \n",
    "        y = np.dot(phi_of_x,self.W_LSR)\n",
    "        if flagSTD>0: y=self.datascalerT.unscale(y) # scale prediction back to original range?\n",
    "        return y                  # return prediction y for data vector x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------\n",
    "# KNN regression \n",
    "# -----------------------------------------------------------------------------------------\n",
    "class KNNRegressifier(Regressifier): \n",
    "    \"\"\"\n",
    "    Class for fast K-Nearest-Neighbor-Regression using KD-trees \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,K,flagKLinReg=0):\n",
    "        \"\"\"\n",
    "        Constructor of class KNNRegressifier\n",
    "        :param K: number of nearest neighbors that are used to compute prediction \n",
    "        :flagKLinReg: if >0 then the do a linear (least squares) regression on the the K nearest neighbors and their target values\n",
    "                      otherwise just take the mean of the K nearest neighbors target vectors\n",
    "        :returns: -\n",
    "        \"\"\"\n",
    "        self.K = K                                 # K is number of nearest-neighbors used for majority decision\n",
    "        self.X, self.T = [],[]                     # initially no data is stored\n",
    "        self.flagKLinReg=flagKLinReg               # if flag is set then do a linear regression of the KNN (otherwise just return mean T of the KNN)\n",
    "\n",
    "    def fit(self,X,T): # train/compute regression with lists of data vectors X and target values T\n",
    "        \"\"\"\n",
    "        Train regressifier by stroing X and T and by creating a KD-Tree based on X   \n",
    "        :param X: Data matrix of size NxD, contains in each row a data vector of size D\n",
    "        :param T: Target vector matrix of size NxK, contains in each row a target vector of size K\n",
    "        :returns: -\n",
    "        \"\"\"\n",
    "        self.X, self.T = np.array(X),np.array(T)   # just store feature vectors X and corresponding class labels T\n",
    "        self.N, self.D = self.X.shape              # store data number N and dimension D\n",
    "        self.kdtree = scipy.spatial.KDTree(self.X) # do an indexing of the feature vectors\n",
    "\n",
    "    def predict(self,x,K=None,flagKLinReg=None):   # predict a target value given data vector x \n",
    "        \"\"\"\n",
    "        predicts the target value y(x) for a test vector x\n",
    "        :param x: test data vector of size D\n",
    "        :param K: number of nearest neighbors that are used to compute prediction \n",
    "        :flagKLinReg: if >0 then the do a linear (least squares) regression on the the K nearest neighbors and their target values\n",
    "                      otherwise just take the mean of the K nearest neighbors target vectors\n",
    "        :returns: predicted target vector of size K\n",
    "        \"\"\"\n",
    "        if(K==None): K=self.K                      # do a K-NN search...\n",
    "        if(flagKLinReg==None): flagKLinReg=self.flagKLinReg # if flag >0 then do a regression on the K Nearest Neighbors to get the prediction\n",
    "        nn = self.kdtree.query(x,K)                # get indexes of K nearest neighbors of x\n",
    "        if K==1: idxNN=[nn[1]]                     # cast nearest neighbor indexes nn as a list idxNN\n",
    "        else: idxNN=nn[1]\n",
    "        t_out=0\n",
    "        if(self.flagKLinReg==0):\n",
    "            # just take mean value of KNNs\n",
    "            t_out=np.mean([self.T[i] for i in idxNN])\n",
    "        else:\n",
    "            # do a linear regression of the KNNs\n",
    "            lsr=LSRegressifier(lmbda=0.0001,phi=lambda x:phi_polynomial(x,1),flagSTD=1)\n",
    "            lsr.fit(self.X[idxNN],self.T[idxNN])\n",
    "            t_out=lsr.predict(x)\n",
    "        return t_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------\n",
      "Example: 1D-linear regression problem\n",
      "-----------------------------------------\n",
      "X= [[ 0. ]\n",
      " [ 0.5]\n",
      " [ 1. ]\n",
      " [ 1.5]\n",
      " [ 2. ]\n",
      " [ 2.5]\n",
      " [ 3. ]\n",
      " [ 3.5]\n",
      " [ 4. ]\n",
      " [ 4.5]\n",
      " [ 5. ]\n",
      " [ 5.5]\n",
      " [ 6. ]\n",
      " [ 6.5]\n",
      " [ 7. ]\n",
      " [ 7.5]\n",
      " [ 8. ]\n",
      " [ 8.5]\n",
      " [ 9. ]\n",
      " [ 9.5]\n",
      " [10. ]\n",
      " [10.5]\n",
      " [11. ]\n",
      " [11.5]\n",
      " [12. ]\n",
      " [12.5]\n",
      " [13. ]\n",
      " [13.5]\n",
      " [14. ]\n",
      " [14.5]\n",
      " [15. ]\n",
      " [15.5]\n",
      " [16. ]\n",
      " [16.5]\n",
      " [17. ]\n",
      " [17.5]\n",
      " [18. ]\n",
      " [18.5]\n",
      " [19. ]\n",
      " [19.5]\n",
      " [20. ]\n",
      " [20.5]\n",
      " [21. ]\n",
      " [21.5]\n",
      " [22. ]\n",
      " [22.5]\n",
      " [23. ]\n",
      " [23.5]\n",
      " [24. ]\n",
      " [24.5]\n",
      " [25. ]\n",
      " [25.5]\n",
      " [26. ]\n",
      " [26.5]\n",
      " [27. ]\n",
      " [27.5]\n",
      " [28. ]\n",
      " [28.5]\n",
      " [29. ]\n",
      " [29.5]\n",
      " [30. ]\n",
      " [30.5]\n",
      " [31. ]\n",
      " [31.5]\n",
      " [32. ]\n",
      " [32.5]\n",
      " [33. ]\n",
      " [33.5]\n",
      " [34. ]\n",
      " [34.5]\n",
      " [35. ]\n",
      " [35.5]\n",
      " [36. ]\n",
      " [36.5]\n",
      " [37. ]\n",
      " [37.5]\n",
      " [38. ]\n",
      " [38.5]\n",
      " [39. ]\n",
      " [39.5]\n",
      " [40. ]\n",
      " [40.5]\n",
      " [41. ]\n",
      " [41.5]\n",
      " [42. ]\n",
      " [42.5]\n",
      " [43. ]\n",
      " [43.5]\n",
      " [44. ]\n",
      " [44.5]\n",
      " [45. ]\n",
      " [45.5]\n",
      " [46. ]\n",
      " [46.5]\n",
      " [47. ]\n",
      " [47.5]\n",
      " [48. ]\n",
      " [48.5]\n",
      " [49. ]\n",
      " [49.5]]\n",
      "T= [[  5.46348334]\n",
      " [  5.46542133]\n",
      " [  5.42878956]\n",
      " [  6.26282968]\n",
      " [  7.66419518]\n",
      " [  8.98611821]\n",
      " [  9.22002274]\n",
      " [ 13.11326444]\n",
      " [ 13.8952383 ]\n",
      " [ 12.08606849]\n",
      " [ 15.23969302]\n",
      " [ 16.31069235]\n",
      " [ 17.5803503 ]\n",
      " [ 17.91895017]\n",
      " [ 18.49214106]\n",
      " [ 19.65769777]\n",
      " [ 20.72695751]\n",
      " [ 20.99696436]\n",
      " [ 22.56784663]\n",
      " [ 22.30311518]\n",
      " [ 24.21447621]\n",
      " [ 24.93768043]\n",
      " [ 26.3705955 ]\n",
      " [ 28.81485   ]\n",
      " [ 28.19157091]\n",
      " [ 28.51501857]\n",
      " [ 28.89912775]\n",
      " [ 31.924139  ]\n",
      " [ 32.29292346]\n",
      " [ 33.03364145]\n",
      " [ 35.06404197]\n",
      " [ 35.2764552 ]\n",
      " [ 36.1778876 ]\n",
      " [ 36.78702418]\n",
      " [ 38.66935704]\n",
      " [ 38.86335407]\n",
      " [ 39.57640629]\n",
      " [ 40.07590352]\n",
      " [ 42.57132917]\n",
      " [ 42.92625799]\n",
      " [ 45.58489211]\n",
      " [ 44.92844919]\n",
      " [ 46.42603482]\n",
      " [ 47.11234762]\n",
      " [ 49.12991792]\n",
      " [ 49.61595162]\n",
      " [ 51.67167255]\n",
      " [ 50.74278304]\n",
      " [ 52.10071027]\n",
      " [ 53.13169208]\n",
      " [ 52.50377535]\n",
      " [ 55.23807249]\n",
      " [ 56.68892269]\n",
      " [ 56.93271305]\n",
      " [ 57.24158068]\n",
      " [ 57.05960175]\n",
      " [ 60.68686852]\n",
      " [ 59.30924457]\n",
      " [ 62.23228251]\n",
      " [ 63.29545878]\n",
      " [ 64.24880396]\n",
      " [ 65.41897908]\n",
      " [ 66.44615954]\n",
      " [ 68.17310142]\n",
      " [ 68.89548285]\n",
      " [ 69.60409877]\n",
      " [ 71.05381529]\n",
      " [ 70.2312292 ]\n",
      " [ 71.18259106]\n",
      " [ 72.6654215 ]\n",
      " [ 72.27251624]\n",
      " [ 74.01009913]\n",
      " [ 78.48567155]\n",
      " [ 76.06892795]\n",
      " [ 77.6609173 ]\n",
      " [ 78.52003927]\n",
      " [ 79.41747291]\n",
      " [ 81.95227826]\n",
      " [ 82.05541674]\n",
      " [ 82.85960402]\n",
      " [ 85.35741427]\n",
      " [ 85.60512554]\n",
      " [ 86.10122057]\n",
      " [ 87.81455946]\n",
      " [ 89.5437667 ]\n",
      " [ 90.43297604]\n",
      " [ 89.35121163]\n",
      " [ 92.84468255]\n",
      " [ 90.19288265]\n",
      " [ 93.09592225]\n",
      " [ 94.37634021]\n",
      " [ 96.10375033]\n",
      " [ 95.49363101]\n",
      " [ 98.20245574]\n",
      " [ 96.01076591]\n",
      " [ 99.57465465]\n",
      " [100.56160475]\n",
      " [102.53576757]\n",
      " [100.85191777]\n",
      " [103.37330157]]\n",
      "phi(4)= [ 1  4 16]\n",
      "phi([1,2])= [1. 1. 2. 1. 2. 4.]\n",
      "\n",
      "-----------------------------------------\n",
      "Do a Least-Squares-Regression\n",
      "-----------------------------------------\n",
      "lsr.W_LSR= [[4.57440346]\n",
      " [1.96899905]\n",
      " [0.00051321]]\n",
      "prediction of x= [3.1415] is y= [10.76507889]\n",
      "LSRRegression cross-validation: absolute errors (E,sd,min,max)= (0.7773417108810148, 0.6012177636487633, 0.003718505400076566, 2.605355032875579)   relative errors (E,sd,min,max)= (0.02638192129568419, 0.041639942989249557, 0.00019056370903977393, 0.2372105985353517)\n",
      "\n",
      "-----------------------------------------\n",
      "Do a KNN-Regression\n",
      "-----------------------------------------\n",
      "prediction of x= [3.1415] is y= 10.57576777251669\n",
      "KNNRegression cross-validation: absolute errors (E,sd,min,max)= (1.3710240697505864, 1.0882361992189327, 0.04002432520232446, 5.1741350499228105)   relative errors (E,sd,min,max)= (0.05698234889207985, 0.130297855504353, 0.0005407954545517795, 0.8096962114851579)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjoern/tools/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:57: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(\"\\n-----------------------------------------\")\n",
    "    print(\"Example: 1D-linear regression problem\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    # (i) generate data\n",
    "    N=100\n",
    "    w0,w1=4,2                 # parameters of line\n",
    "    X=np.zeros((N,1))         # x data: allocate Nx1 matrix as numpy ndarray\n",
    "    X[:,0]=np.arange(0,50.0,50.0/N)  # equidistant sampling of the interval [0,50)\n",
    "    T=np.zeros((N,1))         # target values: allocate Nx1 matrix as numpy ndarray\n",
    "    sd_noise = 1.0            # noise power (=standard deviation)\n",
    "    T=T+w1*X+w0 + np.random.normal(0,sd_noise,T.shape)  # generate noisy target values on line y=w0+w1*x\n",
    "    par_lambda = 0            # regularization parameter\n",
    "    print(\"X=\",X)\n",
    "    print(\"T=\",T)\n",
    "\n",
    "    # (ii) define basis functions (phi should return list of basis functions; x should be a list)\n",
    "    deg=2;                                # degree of polynomial\n",
    "    phi=lambda x: phi_polynomial(x,deg)   # define phi by polynomial basis-functions up to degree deg \n",
    "    print(\"phi(4)=\", phi([4]))            # print basis function vector [1, x, x*x ...] for x=4\n",
    "    print(\"phi([1,2])=\", phi([1,2]))      # print basis function vector for two-dim. inputs (yields many output components) \n",
    "    \n",
    "    # (iii) compute LSR regression\n",
    "    print(\"\\n-----------------------------------------\")\n",
    "    print(\"Do a Least-Squares-Regression\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    lmbda=0;\n",
    "    lsr = LSRRegressifier(lmbda,phi)\n",
    "    lsr.fit(X,T)\n",
    "    print(\"lsr.W_LSR=\",lsr.W_LSR)           # weight vector (should be approximately [w0,w1]=[4,2])\n",
    "    x=np.array([3.1415]).T\n",
    "    print(\"prediction of x=\",x,\"is y=\",lsr.predict(x))\n",
    "\n",
    "    # do S-fold crossvalidation\n",
    "    S=3\n",
    "    err_abs,err_rel = lsr.crossvalidate(S,X,T)\n",
    "    print(\"LSRRegression cross-validation: absolute errors (E,sd,min,max)=\", err_abs, \"  relative errors (E,sd,min,max)=\", err_rel)\n",
    "\n",
    "    # (iv) compute KNN-regression\n",
    "    print(\"\\n-----------------------------------------\")\n",
    "    print(\"Do a KNN-Regression\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    K=5;\n",
    "    knnr = KNNRegressifier(K)\n",
    "    knnr.fit(X,T)\n",
    "    print(\"prediction of x=\",x,\"is y=\",knnr.predict(x))\n",
    "\n",
    "    # do S-fold crossvalidation\n",
    "    err_abs,err_rel = knnr.crossvalidate(S,X,T)\n",
    "    print(\"KNNRegression cross-validation: absolute errors (E,sd,min,max)=\", err_abs, \"  relative errors (E,sd,min,max)=\", err_rel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
