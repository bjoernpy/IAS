{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 2.3\n",
    "**Thema: Lineare Regression auf Airfoil-Noise-Daten**<br>\n",
    "Betrachten Sie nun die Excel-Datensammlung airfoil_self_noise.xls (siehe Vorlesungsverzeichnis): Sie besteht aus N = 1503 Datensätzen der Dimension D = 6, welche den Schalldruck (Spalte 6) einer Flugzeugtragfläche in Abhängigkeit verschiedener Parameter (wie z. B. Anströmwinkel und -geschwindigkeit; Spalten 1-5) darstellen. Sie sollen auf die-\n",
    "sen Daten ein Regressionsmodell lernen, um für neue Parametersätze den resultierenden Schalldruck vorhersagen zu können.\n",
    "\n",
    "## a)\n",
    "Vervollständigen Sie das Programmgerüst V2A3_regression_airfoilnoise.py um eine Least-Squares-Regression auf den Daten zu berechnen. Optimieren Sie die Hyper-Parameter um bei einer S = 3-fachen Kreuzvalidierung möglichst kleine Fehlerwerte zu erhalten.\n",
    "   - Welche Bedeutung haben jeweils die Hyper-Parameter lmbda, deg, flagSTD?\n",
    "       - lmbda: Regularisierungs-Koeffizient\n",
    "       - deg: Grad der Basisfunktion\n",
    "       - flagSTD: falls >0 werden die Daten standardisiert\n",
    "   - Was passiert ohne Skalierung der Daten (flagSTD=0) bei höheren Polynomgraden (achten Sie auf die Werte von maxZ)?\n",
    "       - EXCEPTION DUE TO BAD CONDITION:flagOK= 0  maxZ= 2.1992434255935223e-06 <br> der Fehlerwert geht hoch ohne die Sḱalierung\n",
    "   - Geben Sie Ihre optimalen Hyper-Parameter sowie die resultierenden Fehler-Werte an.\n",
    "       - lmbda = 4, deg = 5, flagSTD = 1\n",
    "       - mean = 0.017310251510489143\n",
    "   - Welche Prognosen ergibt Ihr Modell für die neuen Datenvektoren x_test_1=[1250,11,0.2,69.2,0.0051] bzw. x_test_2=[1305,8,0.1,57.7,0.0048]?\n",
    "       - y(x_test_1) = 130.54643295319002\n",
    "       - y(x_test_2) = 132.04456194433678\n",
    "   - Welchen Polynomgrad und wieviele Basisfunktionen verwendet Ihr Modell?\n",
    "       - Grad 5\n",
    "       - 252 Basisfunktionen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)\n",
    "Vervollständigen Sie das Programmgerüst V2A3_regression_airfoilnoise.py um eine KNN-Regression auf den Daten zu berechnen. Optimieren Sie die Hyper-Parameter um bei einer S = 3-fachen Kreuzvalidierung möglichst kleine Fehlerwerte zu erhalten.\n",
    "   - Welche Bedeutung haben jeweils die Hyper-Parameter K und flagKLinReg?\n",
    "       - K: Anzahl der Nearest Neigbors die mit einbezogen werden\n",
    "       - flagKLinReg: falls >0 wird und K>D wird lineare Regression verwendet\n",
    "   - Geben Sie Ihre optimalen Hyper-Parameter sowie die resultierenden Fehler-Werte an.\n",
    "       - lmbda = 4, K = 20, flagKLinReg = 1, deg = 5, flagSTD = 1\n",
    "       - mean = 0.02379121048873416\n",
    "   - Welche Prognosen ergibt Ihr Modell für die neuen Datenvektoren x_test_1=[1250,11,0.2,69.2,0.0051] bzw. x_test_2=[1305,8,0.1,57.7,0.0048]?\n",
    "       - y(x_test_1) = -1937.5523974696957\n",
    "       - y(x_test_2) = 132.04456194433678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c)\n",
    "Vergleichen Sie die beiden Modelle. Welches liefert die besseren Ergebnisse?\n",
    "\n",
    "Die Least Squares Methode liefert bessere Ergebnisse. <br>\n",
    "KNN liefert ohne und mit dem Flag für lineare Regression schlechtere Ergebnisse.\n",
    "Mit Flag läuft der KNN ebenfalls mit Least Squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to log outputs start with: python V2A3_regression_airfoilnoise.py >V2A3_regression_airfoilnoise.log\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from V2A2_Regression import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***** MAIN PROGRAM ********\n",
    "# (I) Hyper-Parameters\n",
    "S=3;               # S-fold cross-validation\n",
    "lmbda=4;           # regularization parameter (lambda>0 avoids also singularities)\n",
    "K=2;               # K for K-Nearest Neighbors\n",
    "flagKLinReg = 0;   # if flag==1 and K>=D then do a linear regression of the KNNs to make prediction\n",
    "deg=5;             # degree of basis function polynomials \n",
    "flagSTD=1;         # if >0 then standardize data before training (i.e., scale X to mean value 0 and standard deviation 1)\n",
    "N_pred=5;          # number of predictions on the training set for testing\n",
    "x_test_1 = [1250,11,0.2,69.2,0.0051];   # REPLACE dummy code: define test vector 1\n",
    "x_test_2 = [1305,8,0.1,57.7,0.0048];   # REPLACE dummy code: define test vector 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set  ../../DATA/AirfoilSelfNoise/airfoil_self_noise.xls  has size N= 1502  and dimensionality D= 5\n",
      "X= [[1000.            0.            0.3048       71.3           0.00266337]\n",
      " [1250.            0.            0.3048       71.3           0.00266337]\n",
      " [1600.            0.            0.3048       71.3           0.00266337]\n",
      " ...\n",
      " [4000.           15.6           0.1016       39.6           0.0528487 ]\n",
      " [5000.           15.6           0.1016       39.6           0.0528487 ]\n",
      " [6300.           15.6           0.1016       39.6           0.0528487 ]]\n",
      "T= [125.201 125.951 127.591 ... 106.604 106.224 104.204]\n",
      "x_test_1= [1250, 11, 0.2, 69.2, 0.0051]\n",
      "x_test_2= [1305, 8, 0.1, 57.7, 0.0048]\n",
      "number of basis functions M= 252\n"
     ]
    }
   ],
   "source": [
    "# (II) Load data \n",
    "fname='../../DATA/AirfoilSelfNoise/airfoil_self_noise.xls'\n",
    "airfoil_data = pd.read_excel(fname,0); # load data as pandas data frame \n",
    "T = airfoil_data.values[:,5]           # target values = noise load (= column 5 of data table)\n",
    "X = airfoil_data.values[:,:5]          # feature vectors (= column 0-4 of data table)\n",
    "N,D=X.shape                            # size and dimensionality of data set\n",
    "idx_perm = np.random.permutation(N)    # get random permutation for selection of test vectors \n",
    "print(\"Data set \",fname,\" has size N=\", N, \" and dimensionality D=\",D)\n",
    "print(\"X=\",X)\n",
    "print(\"T=\",T)\n",
    "print(\"x_test_1=\",x_test_1)\n",
    "print(\"x_test_2=\",x_test_2)\n",
    "print(\"number of basis functions M=\", len(phi_polynomial(X[1],deg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### Least Squares Regression with regularization lambda= 4  ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bjoer\\Documents\\Praktikum\\Aufgabe 2\\Abgabe\\V2A2_Regression.py:322: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  self.W_LSR = np.linalg.lstsq(PHI.T.dot(PHI) + np.dot(lmbda,np.eye(self.M)), PHI.T.dot(T))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsr.W_LSR= [-0.29232085 -1.53595232 -0.31947921 -0.71582724  0.26783336 -0.49238806\n",
      "  0.36593902 -0.55739161  0.10486601 -0.03292355  0.16969493  0.01386179\n",
      " -0.02385756  0.1213603   0.00986892  0.00270992 -0.0523817   0.02856123\n",
      " -0.12391539 -0.12540552 -0.00641665  0.21542228  0.18849836  0.73746304\n",
      " -0.05511848  0.73652013  0.16539471  0.0870961  -0.0868486   0.32536833\n",
      "  0.16486434 -0.16239675  0.14711298 -0.24586486 -0.01045699  0.16436901\n",
      " -0.1041675  -0.13037834 -0.0666859  -0.09124863  0.01428912 -0.06590659\n",
      " -0.17289218 -0.11145656 -0.13902407 -0.12881238  0.18380523  0.06229881\n",
      "  0.02452736 -0.09228024  0.00105904 -0.24518279  0.08586138 -0.02458379\n",
      " -0.11132804 -0.24116303 -0.14961373  0.13712723 -0.25624705  0.02217059\n",
      " -0.53911543 -0.1086547   0.45811671 -0.05603751 -0.11314852 -0.25850148\n",
      " -0.00481869 -0.07414863  0.02165336  0.05184426 -0.20601768  0.01537968\n",
      " -0.03815796  0.04678697 -0.06887814  0.00490531 -0.06447269 -0.09827021\n",
      "  0.00393971  0.05471029 -0.18367597 -0.5566546   0.00165019 -0.24703325\n",
      "  0.04427584 -0.05050766 -0.12291212  0.02794405 -0.03376836 -0.01952035\n",
      " -0.16843117 -0.06790808  0.02443332 -0.08604159  0.01365519  0.04302688\n",
      "  0.01749336 -0.01197245 -0.06234881  0.01229199  0.03660624 -0.23623893\n",
      "  0.00928599  0.00761404 -0.01248147 -0.00487283 -0.05719881 -0.01130275\n",
      "  0.02689022  0.09519957  0.0112916   0.05559121  0.08658775 -0.1911466\n",
      "  0.06853203  0.10672549  0.05844025 -0.01624257 -0.01731325 -0.09161605\n",
      "  0.00395505  0.02001948  0.0523374   0.09149145  0.0437023   0.08729404\n",
      "  0.01377998  0.01595159  0.0169005  -0.0060408  -0.00108521  0.25738074\n",
      "  0.04551564  0.00171532 -0.22669881  0.0399408   0.00538604 -0.23258289\n",
      "  0.01042347 -0.03110796  0.12550915  0.09898381  0.11691481 -0.07972088\n",
      "  0.02048934 -0.06995247  0.0372224   0.08522994 -0.02012763  0.06239476\n",
      "  0.04840207  0.049237    0.00318107  0.0670953  -0.01104339  0.00804941\n",
      " -0.0905153  -0.00319461  0.11675871 -0.03466443 -0.00747009 -0.03636881\n",
      " -0.12948903 -0.00837148 -0.10266307 -0.21781555  0.03938799 -0.03956544\n",
      " -0.04380973  0.02381157 -0.03853733 -0.10641113  0.0987567   0.12064493\n",
      " -0.05828632  0.04860342  0.11976753  0.0140862   0.15465205 -0.00685096\n",
      "  0.15402907  0.19475459  0.01324305  0.10204879  0.00377758 -0.0739674\n",
      "  0.22601404  0.01134689  0.08173715  0.03394736  0.06463549  0.04566069\n",
      " -0.01101087  0.03360381 -0.01167155  0.00093578  0.10070629  0.16225651\n",
      "  0.07955404  0.00383853 -0.09735865  0.1111889  -0.01872266  0.08739393\n",
      " -0.04953025 -0.04841081 -0.0547443   0.04642525 -0.02221381 -0.01482694\n",
      " -0.0294038  -0.09247937  0.02249743 -0.09214979 -0.06813011 -0.07825777\n",
      "  0.22411611 -0.01651091  0.11746839  0.01141397  0.02702874  0.09431681\n",
      "  0.02693252  0.02627242 -0.11918477 -0.05599743 -0.01187817 -0.01213573\n",
      " -0.06989115 -0.05189371 -0.03514172 -0.05764579 -0.05835545 -0.04305671\n",
      " -0.04601109  0.01128126  0.00822572  0.00462616 -0.0114146   0.04141149\n",
      "  0.03981465  0.04819665 -0.03812479 -0.00855174 -0.09640577 -0.09536511\n",
      " -0.02256816  0.02463319 -0.03748524  0.02972964  0.0548926   0.02478219]\n",
      "III.1) Some predictions on the training data:\n",
      "Prediction for X[ 689 ]= [4000.           9.9          0.1524      31.7          0.0252785]  is y= 110.17293693814719 , whereas true value is T[ 689 ]= 111.459\n",
      "Prediction for X[ 1394 ]= [315.          8.9         0.1016     39.6         0.0124596]  is y= 130.78937240215558 , whereas true value is T[ 1394 ]= 135.38\n",
      "Prediction for X[ 46 ]= [12500.             0.             0.3048        39.6\n",
      "     0.00310138]  is y= 109.23124919642834 , whereas true value is T[ 46 ]= 109.619\n",
      "Prediction for X[ 688 ]= [3150.           9.9          0.1524      31.7          0.0252785]  is y= 111.90525369455263 , whereas true value is T[ 688 ]= 112.209\n",
      "Prediction for X[ 455 ]= [1250.           0.           0.1524      71.3          0.0015988]  is y= 127.27650267508787 , whereas true value is T[ 455 ]= 128.927\n",
      "III.2) Some predicitions for new test vectors:\n",
      "Prediction for x_test_1 is y= 130.49946993437166\n",
      "Prediction for x_test_2 is y= 131.69355772247633\n",
      "III.3) S= 3 fold Cross Validation:\n",
      "absolute errors (E,sd,min,max)= (2.1473010324576696, 2.6710969835031304, 5.9320798499129523e-05, 64.54727330997825) \n",
      "relative errors (E,sd,min,max)= (0.017310251510489143, 0.022882679881095023, 4.3661253366647667e-07, 0.588730853444775)\n"
     ]
    }
   ],
   "source": [
    "# (III) Do least-squares regression with regularization \n",
    "print(\"\\n#### Least Squares Regression with regularization lambda=\", lmbda, \" ####\")\n",
    "#lsr = None  # REPLACE dummy code: Create and fit Least-Squares Regressifier using polynomial basis function of degree deg and flagSTD for standardization of data\n",
    "phi=lambda x: phi_polynomial(x,deg)\n",
    "lsr = LSRRegressifier(lmbda, phi, flagSTD)\n",
    "lsr.fit(X,T)\n",
    "print(\"lsr.W_LSR=\",lsr.W_LSR)    # REPLACE dummy code: print weight vector for least squares regression  \n",
    "print(\"III.1) Some predictions on the training data:\")\n",
    "for i in range(N_pred): \n",
    "    n=idx_perm[i]\n",
    "    print(\"Prediction for X[\",n,\"]=\",X[n],\" is y=\",lsr.predict(X[n]),\", whereas true value is T[\",n,\"]=\",T[n])   # REPLACE dummy code: compute prediction for X[n]\n",
    "print(\"III.2) Some predicitions for new test vectors:\")\n",
    "print(\"Prediction for x_test_1 is y=\", lsr.predict(x_test_1))    # REPLACE dummy code: compute prediction for x_test_1\n",
    "print(\"Prediction for x_test_2 is y=\", lsr.predict(x_test_2))    # REPLACE dummy code: compute prediction for x_test_2\n",
    "print(\"III.3) S=\",S,\"fold Cross Validation:\")\n",
    "err_abs,err_rel = lsr.crossvalidate(S,X,T)                 # REPLACE dummy code: do cross validation!! \n",
    "print(\"absolute errors (E,sd,min,max)=\", err_abs, \"\\nrelative errors (E,sd,min,max)=\", err_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#### KNN regression with flagKLinReg= 0  ####\n",
      "IV.1) Some predictions on the training data:\n",
      "Prediction for X[ 555 ]= [630.           5.4          0.1524      55.5          0.00433288]  is y= 132.415 , whereas true value is T[ 555 ]= 131.807\n",
      "Prediction for X[ 458 ]= [2500.           0.           0.1524      71.3          0.0015988]  is y= 130.361 , whereas true value is T[ 458 ]= 128.127\n",
      "Prediction for X[ 1358 ]= [2500.           6.7          0.1016      55.5          0.0052139]  is y= 121.6335 , whereas true value is T[ 1358 ]= 124.273\n",
      "Prediction for X[ 904 ]= [400.         15.4         0.0508     39.6         0.0282593]  is y= 122.9185 , whereas true value is T[ 904 ]= 125.353\n",
      "Prediction for X[ 302 ]= [1250.            4.            0.2286       71.3           0.00400603]  is y= 130.5565 , whereas true value is T[ 302 ]= 131.718\n",
      "IV.2) Some predicitions for new test vectors:\n",
      "Prediction for x_test_1 is y= 129.435\n",
      "Prediction for x_test_2 is y= 128.988\n",
      "IV.3) S= 3 fold Cross Validation:\n",
      "absolute errors (E,sd,min,max)= (5.315593874833551, 4.265595314723435, 0.0010000000000047748, 21.251000000000005) \n",
      "relative errors (E,sd,min,max)= (0.04302675285476087, 0.03500201928518538, 7.803720814121417e-06, 0.1740411637911433)\n"
     ]
    }
   ],
   "source": [
    "# (IV) Do KNN regression  \n",
    "print(\"\\n#### KNN regression with flagKLinReg=\", flagKLinReg, \" ####\")\n",
    "knnr = KNNRegressifier(K,flagKLinReg)                                    # REPLACE dummy code: Create and fit KNNRegressifier\n",
    "knnr.fit(X,T)\n",
    "print(\"IV.1) Some predictions on the training data:\")\n",
    "for i in range(N_pred): \n",
    "    n=idx_perm[i]\n",
    "    print(\"Prediction for X[\",n,\"]=\",X[n],\" is y=\",knnr.predict(X[n]),\", whereas true value is T[\",n,\"]=\",T[n])   # REPLACE dummy code: compute prediction for X[n]\n",
    "print(\"IV.2) Some predicitions for new test vectors:\")\n",
    "print(\"Prediction for x_test_1 is y=\", knnr.predict(x_test_1))    # REPLACE dummy code: compute prediction for x_test_1\n",
    "print(\"Prediction for x_test_2 is y=\", knnr.predict(x_test_2))    # REPLACE dummy code: compute prediction for x_test_2\n",
    "print(\"IV.3) S=\",S,\"fold Cross Validation:\")\n",
    "err_abs,err_rel = knnr.crossvalidate(S,X,T)                   # REPLACE dummy code: do cross validation!! \n",
    "print(\"absolute errors (E,sd,min,max)=\", err_abs, \"\\nrelative errors (E,sd,min,max)=\", err_rel )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
