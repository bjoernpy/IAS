{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)\n",
    "<i>Betrachten Sie den Aufbau des Moduls durch Eingabe von pydoc V1A2_Classifier. Welche Klassen gehören zu dem Modul und welchen Zweck haben sie jeweils?</i> <br>\n",
    "    Klassen: Classifier, KNNClassifier, FastKNNClassifier <br>\n",
    "\n",
    "<i>Betrachten Sie nun die Basis-Klasse Classifier im Quelltext: Wozu dienen jeweils die Methoden init(self,C), fit(self,X,T), predict(self,x) und crossvalidate(self,S,X,T) ?</i> <br>\n",
    "    init: Konstruktor <br>\n",
    "    fit: Legt die Matrix an welche die Vektoren beinhaltet und die zugehörigen Labels <br>\n",
    "    predict: Bekommt Testdatenvektor übergeben und liefert das Label für die am wahrscheinlichsten zugehörige Klasse <br>\n",
    "    crossvalidate: führt eine Kreuzvalidierung durch <br>\n",
    "\n",
    "\n",
    "## b)\n",
    "<i>Wie “lernt” ein k-NN-Klassifikator?</i> <br>\n",
    "Legt die Matrix an welche die Vektoren beinhaltet und die zugehörigen Labels <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import scipy.spatial\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------- \n",
    "# Base class for classifiers\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "class Classifier:\n",
    "    \"\"\"\n",
    "    Abstract base class for a classifier.\n",
    "    Inherit from this class to implement a concrete classification algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,C=2): \n",
    "        \"\"\"\n",
    "        Constructor of class Classifier\n",
    "        Should be called by the constructors of derived classes\n",
    "        :param C: Number of different classes\n",
    "        \"\"\"\n",
    "        self.C = C            # set C=number of different classes \n",
    "\n",
    "    def fit(self,X,T):    \n",
    "        \"\"\" \n",
    "        Train classier by training data X, T, should be overwritten by any derived class\n",
    "        :param X: Data matrix, contains in each row a data vector\n",
    "        :param T: Vector of class labels, must have same length as X, each label should be integer in 0,1,...,C-1\n",
    "        :returns: - \n",
    "        \"\"\"\n",
    "        shapeX,shapeT=X.shape,T.shape  # X must be a N x D matrix; T must be a N x 1 matrix; N is number of data vectors; D is dimensionality\n",
    "        assert len(shapeX)==2, \"Classifier.fit(self,X,T): X must be two-dimensional array!\"\n",
    "        assert len(shapeT)==1, \"Classifier.fit(self,X,T): T must be one-dimensional array!\"\n",
    "        assert shapeX[0]==shapeT[0], \"Classifier.fit(self,X,T): Data matrix X and class labels T must have same length!\"\n",
    "        self.C=max(T)+1;       # number of different integer-type class labels (assuming that T(i) is in {0,1,...,C-1})\n",
    "\n",
    "    def predict(self,x):\n",
    "        \"\"\" \n",
    "        Implementation of classification algorithm, should be overwritten in any derived class\n",
    "        :param x: test data vector\n",
    "        :returns: label of most likely class that test vector x belongs to (and possibly additional information)\n",
    "        \"\"\"\n",
    "        return -1,None,None\n",
    "\n",
    "    def crossvalidate(self,S,X,T):    # do a S-fold cross validation \n",
    "        \"\"\"\n",
    "        Do a S-fold cross validation\n",
    "        :param S: Number of parts the data set is divided into\n",
    "        :param X: Data matrix (one data vector per row)\n",
    "        :param T: Vector of class labels; T[n] is label of X[n]\n",
    "        :returns pClassError: probability of a classification error (=1-Accuracy)\n",
    "        :returns pConfErrors: confusion matrix, pConfErrors[i,j] is the probability that a vector from true class j will be mis-classified as class i\n",
    "        \"\"\"\n",
    "        N=len(X)                                            # N=number of data vectors\n",
    "        perm = np.random.permutation(N)                     # do a random permutation of X and T...\n",
    "        Xp,Tp=[X[i] for i in perm], [T[i] for i in perm]    # ... to get a random partition of the data set\n",
    "        idxS = [range(i*N//S,(i+1)*N//S) for i in range(S)] # divide data set into S parts:\n",
    "        C=max(T)+1;                                         # number of different class labels (assuming that t is in {0,1,...,C-1})\n",
    "        nC          = np.zeros(C)                           # initialize class probabilities: nC[i]:=N*pr[xn is of class i]\n",
    "        pConfErrors = np.zeros((C,C))                       # initialize confusion error probabilities pr[class i|class j]\n",
    "        pClassError = 0                                     # initialize probability of a classification error\n",
    "        for idxTest in idxS:                                # loop over all possible test data sets\n",
    "            # (i) generate training and testing data sets and train classifier        \n",
    "            idxLearn = [i for i in range(N) if i not in idxTest]                      # remaining indices (not in idxTest) are learning data\n",
    "            if(S<=1): idxLearn=idxTest                                                # if S==1 use entire data set for learning and testing\n",
    "            X_learn, T_learn = [Xp[i] for i in idxLearn], [Tp[i] for i in idxLearn]   # learning data for training the classifier\n",
    "            X_test , T_test  = [Xp[i] for i in idxTest] , [Tp[i] for i in idxTest]    # test data \n",
    "            self.fit(np.array(X_learn),np.array(T_learn))                             # train classifier\n",
    "            # (ii) test classifier\n",
    "            for i in range(len(X_test)):  # loop over all data vectors to be tested\n",
    "                # (ii.a) classify i-th test vector\n",
    "                t_test = self.predict(X_test[i])[0]             # classify test vector\n",
    "                # (ii.b) check for classification errors\n",
    "                t_true = T_test[i]                              # true class label\n",
    "                nC[t_true]=nC[t_true]+1                         # count occurrences of individual classes\n",
    "                pConfErrors[t_test][t_true]=pConfErrors[t_test][t_true]+1  # count conditional class errors\n",
    "                if(t_test!=t_true): pClassError=pClassError+1              # count total number of errors\n",
    "        pClassError=float(pClassError)/float(N)         # probability of a classification error\n",
    "        for i in range(C): \n",
    "            for j in range(C): \n",
    "                pConfErrors[i,j]=float(pConfErrors[i,j])/float(nC[j])   # finally compute confusion error probabilities\n",
    "        self.pClassError,self.pConfErrors=pClassError,pConfErrors       # store error probabilities as object fields\n",
    "        return pClassError, pConfErrors                 # return error probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------- \n",
    "# (Naive) k-nearest-neighbor classifier based on simple look-up-table and exhaustive search\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "class KNNClassifier(Classifier):\n",
    "    \"\"\"\n",
    "    (Naive) k-nearest-neighbor classifier based on simple look-up-table and exhaustive search\n",
    "    Derived from base class Classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,C=2,k=1):\n",
    "        \"\"\"\n",
    "        Constructor of the KNN-Classifier\n",
    "        :param C: Number of different classes\n",
    "        :param k: Number of nearest neighbors that classification is based on\n",
    "        \"\"\"\n",
    "        Classifier.__init__(self,C) # call constructor of base class  \n",
    "        self.k = k                  # k is number of nearest-neighbors used for majority decision\n",
    "        self.X, self.T = [],[]      # initially no data is stored\n",
    "\n",
    "    def fit(self,X,T):\n",
    "        \"\"\"\n",
    "        Train classifier; for naive KNN Classifier this just means to store data matrix X and label vector T\n",
    "        :param X: Data matrix, contains in each row a data vector\n",
    "        :param T: Vector of class labels, must have same length as X, each label should be integer in 0,1,...,C-1\n",
    "        :returns: - \n",
    "        \"\"\"\n",
    "        Classifier.fit(self,X,T);   # call to base class to check for matrix dimensions etc.\n",
    "        self.X, self.T = X,T        # just store the N x D data matrix and the N x 1 label matrix (N is number and D dimensionality of data vectors) \n",
    "        \n",
    "    def getKNearestNeighbors(self, x, k=None, X=None):\n",
    "        \"\"\"\n",
    "        compute the k nearest neighbors for a query vector x given a data matrix X\n",
    "        :param x: the query vector x\n",
    "        :param X: the N x D data matrix (in each row there is data vector) as a numpy array\n",
    "        :param k: number of nearest-neighbors to be returned\n",
    "        :return: list of k line indexes referring to the k nearest neighbors of x in X\n",
    "        \"\"\"\n",
    "        if(k==None): k=self.k                      # per default use stored k \n",
    "        if(X==None): X=self.X                      # per default use stored X\n",
    "            \n",
    "        d=[sum([((x[j] - X[i][j])**2) for j in range(len(X[0]))]) for i in range(len(X))]\n",
    "        return [d.index(sorted(d)[i]) for i in range(k)]                            # REPLACE: Insert/adapt your code from V1A1_KNearestNeighborSearch.py\n",
    "\n",
    "    def predict(self,x,k=None):\n",
    "        \"\"\" \n",
    "        Implementation of classification algorithm, should be overwritten in any derived classes\n",
    "        :param x: test data vector\n",
    "        :param k: search k nearest neighbors (default self.k)\n",
    "        :returns prediction: label of most likely class that test vector x belongs to\n",
    "                             if there are two or more classes with maximum probability then one class is chosen randomly\n",
    "        :returns pClassPosteriori: A-Posteriori probabilities, pClassPosteriori[i] is probability that x belongs to class i\n",
    "        :returns idxKNN: indexes of the k nearest neighbors (ordered w.r.t. descending distance) \n",
    "        \"\"\"\n",
    "        if k==None: k=self.k                       # use default parameter k?\n",
    "        idxKNN = self.getKNearestNeighbors(x,k)    # get indexes of k nearest neighbors of x\n",
    "        \n",
    "        prediction=0                               # REPLACE DUMMY CODE BY YOUR OWN CODE!\n",
    "        prediction=self.T[idxKNN[0]]\n",
    "        \n",
    "        pClassPosteriori=self.C*[1.0/self.C]       # REPLACE DUMMY CODE BY YOUR OWN CODE!\n",
    "        labelSet = set(self.T)\n",
    "        labelList = list(labelSet)\n",
    "        for i in range(self.C) :\n",
    "            pClassPosteriori[i] = len([1 for j in range(k) if labelList[i]==self.T[idxKNN[j]]])/k\n",
    "        \n",
    "        return prediction, pClassPosteriori, idxKNN  # return predicted class, a-posteriori-distribution, and indexes of nearest neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------- \n",
    "# Fast k-nearest-neighbor classifier based on scipy KD trees\n",
    "# ----------------------------------------------------------------------------------------- \n",
    "class FastKNNClassifier(KNNClassifier):\n",
    "    \"\"\"\n",
    "    Fast k-nearest-neighbor classifier based on kd-trees \n",
    "    Inherits from class KNNClassifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,C=2,k=1):\n",
    "        \"\"\"\n",
    "        Constructor of the KNN-Classifier\n",
    "        :param C: Number of different classes\n",
    "        :param k: Number of nearest neighbors that classification is based on\n",
    "        \"\"\"\n",
    "        KNNClassifier.__init__(self,C,k)     # call to parent class constructor  \n",
    "\n",
    "    def fit(self,X,T):\n",
    "        \"\"\"\n",
    "        Train classifier by creating a kd-tree \n",
    "        :param X: Data matrix, contains in each row a data vector\n",
    "        :param T: Vector of class labels, must have same length as X, each label should be integer in 0,1,...,C-1\n",
    "        :returns: - \n",
    "        \"\"\"\n",
    "        KNNClassifier.fit(self,X,T)                # call to parent class method (just store X and T)\n",
    "        self.kdtree = None                         # REPLACE DUMMY CODE BY YOUR OWN CODE! Do an indexing of the feature vectors by constructing a kd-tree\n",
    "        \n",
    "    def getKNearestNeighbors(self, x, k=None):  # realizes fast K-nearest-neighbor-search of x in data set X\n",
    "        \"\"\"\n",
    "        fast computation of the k nearest neighbors for a query vector x given a data matrix X by using the KD-tree\n",
    "        :param x: the query vector x\n",
    "        :param k: number of nearest-neighbors to be returned\n",
    "        :return idxNN: return list of k line indexes referring to the k nearest neighbors of x in X\n",
    "        \"\"\"\n",
    "        if(k==None): k=self.k                      # do a K-NN search...\n",
    "        idxNN = k*[0]                              # REPLACE DUMMY CODE BY YOUR OWN CODE! Compute nearest neighbors using the KD-Tree\n",
    "        return idxNN                               # return indexes of k nearest neighbors\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data matrix X=\n",
      " [[1 2 3]\n",
      " [2 3 4]\n",
      " [3 4 5]\n",
      " [4 5 6]]\n",
      "Class labels T=\n",
      " [0 1 0 1]\n",
      "Test vector x= [1.5 3.6 5.7]\n",
      "Euklidean distances d= [3.178049716414141, 1.8708286933869709, 1.7029386365926402, 2.8809720581775866]\n",
      "\n",
      "Classification with the naive KNN-classifier:\n",
      "Test vector is most likely from class  0\n",
      "A-Posteriori Class Distribution: prob(x is from class i)= [0.3333333333333333, 0.6666666666666666]\n",
      "Indexes of the k= 3  nearest neighbors: idx_knn= [2, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "# *******************************************************\n",
    "# __main___\n",
    "# Module test\n",
    "# *******************************************************\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # (i) Generate dummy data \n",
    "    X = np.array([[1,2,3],[2,3,4],[3,4,5],[4,5,6]]);      # data matrix X: list of data vectors (=database) of dimension D=3\n",
    "    T = np.array([0,1,0,1]);                              # target values aka class labels\n",
    "    x = np.array([1.5,3.6,5.7]);                          # a test data vector\n",
    "    print(\"Data matrix X=\\n\",X)\n",
    "    print(\"Class labels T=\\n\",T)\n",
    "    print(\"Test vector x=\",x)\n",
    "    print(\"Euklidean distances d=\",[LA.norm(x-X[i]) for i in range(len(X))])                     # REPLACE DUMMY CODE (IF YOU WANT) ...\n",
    "\n",
    "    # (ii) Train simple KNN-Classifier\n",
    "    knnc = KNNClassifier()         # construct kNN Classifier\n",
    "    knnc.fit(X,T)                  # train with given data\n",
    "\n",
    "    # (iii) Classify test vector x\n",
    "    k=3\n",
    "    c,pc,idx_knn=knnc.predict(x,k)\n",
    "    print(\"\\nClassification with the naive KNN-classifier:\")\n",
    "    print(\"Test vector is most likely from class \",c)\n",
    "    print(\"A-Posteriori Class Distribution: prob(x is from class i)=\",pc)\n",
    "    print(\"Indexes of the k=\",k,\" nearest neighbors: idx_knn=\",idx_knn)\n",
    "\n",
    "    # (iv) Repeat steps (ii) and (iii) for the FastKNNClassifier (based on KD-Trees)\n",
    "    # INSERT YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
